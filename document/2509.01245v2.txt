arXiv:2509.01245v2 [cs.AI] 3 Sep 2025

Towards Agentic OS: An LLM Agent Framework for
Linux Schedulers
Yusheng Zheng

Yanpeng Hu

yzhen165@ucsc.edu
University of California, Santa Cruz
Santa Cruz, California, USA

huyp@shanghaitech.edu.cn
ShanghaiTech University
Shanghai, China

Wei Zhang

Andi Quinn

wei.13.zhang@uconn.edu
University of Connecticut
Storrs, Connecticut, USA

aquinn1@ucsc.edu
University of California, Santa Cruz
Santa Cruz, California, USA

Abstract
Operating system schedulers suffer from a fundamental
semantic gap, where kernel policies fail to understand
application-specific needs, leading to suboptimal performance. We introduce SchedCP, the first framework that enables fully autonomous Large Language Model (LLM) agents
to safely and efficiently optimize Linux schedulers without
human involvement. Our core insight is that the challenge
is not merely to apply a better LLM, but to architect a decoupled control plane that separates the AI’s role of semantic
reasoning ("what to optimize") from the system’s role of execution ("how to observe and act"). Implemented as Model
Context Protocol(MCP) server, SchedCP provides a stable interface with three key services: a Workload Analysis Engine,
an evolving Scheduler Policy Repository, and an Execution
Verifier that validates all AI-generated code and configure
before deployment with static and dynamic analysis.
We demonstrate this architecture’s power with schedagent, a multi-agent system that autonomously analyzes
workloads, synthesizes custom eBPF scheduling policies, and
deploys them via the sched_ext infrastructure. Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared
to naive agentic approaches, all while maintaining high
success rate. By bridging the semantic gap, SchedCP democratizes expert-level system optimization and represents
a step towards creating truly self-optimizing, applicationaware operating systems. The code is open-sourced in
https://github.com/eunomia-bpf/schedcp

1

Introduction

Operating system schedulers face a fundamental challenge:
kernel policies cannot understand what applications need.
This semantic gap leads to suboptimal performance across
modern computing infrastructure. In cloud platforms, system
administrators who manage schedulers are not the developers who understand application behavior. On personal
devices, regular users lack the expertise to optimize their

systems for gaming or creative workloads. Meanwhile, workloads themselves exhibit increasingly dynamic patterns that
defy manual optimization.
Prior attempts to automate scheduler optimization, such
as those using reinforcement learning [19, 28], have shown
promise but remain fundamentally limited. By mapping numerical state to predefined actions, they cannot grasp the
semantic intent of a workload and miss optimization opportunities that require deeper reasoning. The advent of Large
Language Models (LLMs) Agents, which can automatically
reason and use tools for software development, presents an
opportunity to bridge this semantic gap, yet a naive approach
is impractical. As our motivating experiments reveal, using
a powerful agent to generate a basic scheduler from scratch
was slow, expensive (~$6), and resulted in code that often
degraded system performance. This highlights a critical gap:
existing methods lack semantic understanding, while powerful new models lack the necessary scaffolding for safe,
efficient, and reliable systems integration.
To bridge this gap, we introduce a novel, decoupled architecture consisting of two complementary components that
leverages AI’s unique strengths (semantic reasoning and generative synthesis) while mitigating its weaknesses of cost and
unreliability. The first component is SchedCP, a control plane
framework that acts as a safe, stable interface between AI and
the kernel. SchedCP provides the essential tools any agent
needs to optimize schedulers, including profilers and tracers
for observation, and static and dynamic analysis for validation and safe deployment. The second component is schedagent, our implementation of an autonomous reinforcement
learning policy engine that leverages a multi-agent LLM system. sched-agent uses the capabilities provided by SchedCP
to reason about workloads, synthesize policies, and adapts
its strategy based on performance feedback. By reducing
optimization costs, our approach makes custom scheduler
development economically viable even for short-lived workloads like CI/CD pipelines that previously could not justify
the engineering investment.
This architectural separation is fundamental to our approach. SchedCP embodies our core systems contribution:

Conference’17, July 2017, Washington, DC, USA

a generalizable framework that can work with any future
AI agent, while sched-agent demonstrates the power of this
approach through semantic workload analysis and intelligent policy generation. The name ‘SchedCP ‘ is inspired by
"Context Protocol" (like MCP) and the networking concept
of a "Control Plane," reflecting its role as a control plane for
AI-driven policy orchestration, separate from the data plane
where low-level scheduling decisions execute. Deployed on
the production-ready sched_ext infrastructure, our approach
executes with zero LLM overhead in the critical path and
makes the following contributions:
• The SchedCP interface: A framework that exposes
kernel scheduling related features via the Model Context Protocol (MCP), featuring three core services
(Workload Analysis Engine, Scheduler Policy Repository, and Execution Verifier) that enable any agent
to perform deep semantic analysis of workloads, do
AI-driven scheduler optimization without compromising system stability, and learns from experience and
improve performance over time.
• sched-agent multi-agent system: An autonomous
reinforcement learning policy engine that decomposes scheduler optimization into four specialized
agents (Observation, Planning, Execution, and Learning), demonstrating how LLMs can bridge the semantic gap between application requirements and kernel
scheduling policies.
• Evaluation: We demonstrate that sched-agent
achieves up to 1.79× performance gains on kernel
compilation, 2.11× P99 latency improvement and
1.60× throughput gain on schbench, 20% average
latency reduction for batch workloads, and 13× lower
cost compared to naive approaches, while maintaining
system stability across diverse workloads.
Paper organization: Background (§2), Motivation (§3),
SchedCP (§4), sched-agent (§5), Evaluation (§6), Related
Work (§7), Future Work (§8), and Conclusion (§9).

2

Background

This section reviews the two core technologies for our work:
extensible kernel scheduling and autonomous LLM agents.
2.1

eBPF and sched_ext

Linux’s default Earliest Eligible Virtual Deadline First
(EEVDF) scheduler [17], which replaced CFS in kernel 6.6, is a
one-size-fits-all policy that, while ensuring fairness through
virtual deadlines, is unoptimized for diverse workloads. To
address this, sched_ext [10], introduced in Linux 6.12, enables the dynamic loading of custom schedulers as eBPF
programs, providing hooks for task enqueueing, CPU selection, load balancing, and idle management. This relies on
eBPF [13], which evolved from a simple packet filter into a
general-purpose, in-kernel virtual machine. Now powering

Yusheng Zheng, Yanpeng Hu, Wei Zhang, and Andi Quinn

modern observability and security tools [7, 29] on both Linux
and other systems like Windows and userspace [23, 38], its
verifier guarantees safety through static analysis, preventing
crashes, invalid memory access, and infinite loops. Previous
work also explores using LLM for eBPF code generation[36].
2.2

LLMs and Autonomous Agents

The application of large language models (LLMs) like GPT4 [26] and Claude [1] has evolved from code generation
to system maintenance [12] and fully autonomous agents.
These agents typically use an architecture with an LLM backend, a tool framework, and a control loop, as seen in popular
frameworks like LangChain [9], AutoGen [34] and commercial tools like Cursor Agent [6], Gemini-CLI [25], and Claude
Code [4] for automate software engineering workflows. Further research into multi-agent frameworks like ChatDev [27]
and MetaGPT for software development [14], repo understanding [30], and simulate social behaviors [18] has shown
that role-playing can boost code generation and issue localization [8, 16]. Despite these advances, the tools remain
developer aids, not autonomous low-level system optimizers.

3

Motivation

We motivate our work by examining this semantic gap problem and the practical safety, performance, and cost issues
revealed by our experiments.
3.1

The Semantic Gap Problem

Linux scheduler optimization faces three fundamental barriers. First, a domain knowledge gap exists between developers and users: DevOps engineers lack insight into workload
characteristics (latency-sensitive vs. throughput-oriented),
while edge/personal device users lack both kernel optimization expertise and understanding of application-specific performance targets. Second, technical complexity of scheduler development requires mastering kernel programming
with lock-free structures, eBPF verification constraints, and
CPU/NUMA architectures, limiting innovation to few experts. Third, dynamic workload behavior presents complex challenges: ML training alternates between computeintensive forward propagation and communication-heavy
gradient synchronization, web traffic varies by orders of
magnitude daily, and build system parallelism changes with
dependencies.
Prior RL-based schedulers [19, 20, 28, 35] require extensive training per workload type, lack semantic understanding to transfer knowledge across workloads, and cannot
generate new scheduling code. LLMs uniquely bridge these
gaps by: (1) understanding natural language requirements
and source code semantics without task-specific training, (2)
synthesizing correct eBPF schedulers based on automatic
workload characterization, (3) reasoning about performance

Towards Agentic OS: An LLM Agent Framework for Linux Schedulers

Conference’17, July 2017, Washington, DC, USA

trade-offs and system constraints, and critically, (4) operating in the control plane to generate optimized code that
runs natively with negligible runtime overhead, unlike traditional ML models that would cause unacceptable inference
latency in the scheduler hot path. This control plane separation represents a key architectural insight: LLMs generate
and optimize scheduling policies offline, producing native
eBPF code that executes without any ML inference overhead
during actual scheduling decisions.
3.2

3.3

Challenges in Applying LLM Agents to
Schedulers

Our experiments reveal critical challenges for AI-driven
scheduler optimization, especially when fully automated:
Performance: How do we ensure AI-generated or configured schedulers outperform existing ones rather than degrading performance? Safety: How do we prevent kernel
crashes, soft-lockups, stalls, or starvation while maintaining stability? How can we ensure only minimal privilege
needed when development and deployment? This presents a
fundamental programming language challenge: synthesizing
domain-specific code that must satisfy both general safety
properties (memory safety, termination) and domain-specific
invariants (fairness, liveness)—a problem that requires sophisticated verification techniques beyond standard compiler
checks. Efficiency: The 33-minute generation time and the
$6 cost must drop for practical deployment.

4

"Create plans
for optimization Workload
and give tasks
Profile
to other agents"
New workload
detected:
"Analyze and
generate report"

SchedCP
(System
Interface)

Motivation Experiment

We tested Claude Code[4], the state of the art LLM agent,
with "write a FIFO scheduler in eBPF" from an empty folder,
with all permissions and bash access. Of three attempts, only
one succeeded. The second attempt produced pseudo-code
after 6 minutes trying, and the third generated a scheduler
tracer instead after 8 minutes development. The successful
generation required 33 minutes, 221 LLM API calls, and 15+
iterations, costing $6 (vs. 5 minutes typically for an expert
developer). The generated code, for some workloads, exhibited poor quality with excessive overhead, performing worse
than EEVDF. The agent required root access, could crash
the system during testing, and lacked fallback mechanisms,
which also raises safety concerns.

The SchedCP Framework Design and
Implementation

Our approach to agentic OS optimization is founded on a
clean separation between the systems infrastructure and the
AI logic, as illustrated in Figure 1. We introduce SchedCP, a
stable and secure control plane that acts as an ’API for OS
optimization.’ Our research is motivated by the insight that
AI agents are fundamentally context engineering systems;
like human experts, they need the right tools to gather information and act without being overwhelmed by prohibitive

Optimization

Planning
Agent

Plan
"Generate
code & cfg"

Observation
Agent

Learning
Agent

Validate &
Deploy
Execution
Verifier

sched_ext

"What to
Optimize" Reasoning &
coding & tool
usage

Retrieve &
Update
Scheduler
Policy
Repository

Load eBPF

Read, test,
monitor
Application

"How does this
scheduler perform"

Execution
Agent

Analyze

Workload
Analysis
Engine

sched-agent
(AI Logic)

"How to
Observe &
Act" - Safe
Control Plane

Linux
Kernel

Figure 1. The overall architecture, showing the separation of concerns between SchedCP and sched-agent The
SchedCP framework (bottom) acts as a safe system interface, providing tools to analyze workloads, verify code, and
manage scheduler policies in the Linux kernel via eBPF. The
sched-agent framework (top) contains the AI logic, where
specialized agents for Observation, Planning, Execution, and
Learning collaborate in a closed loop to autonomously create,
deploy, and refine scheduling policies. The red line indicates
the initialization process when SchedCP detects a new workload. The back arrow indicates the optimization loops, where
sched-agent continuously refines scheduler policies based on
optimization plan and observation results. The green arrows
indicate the tool usage by the AI Agents.
costs or irrelevant data. Therefore, as system researchers,
our goal is not to build better AI agents, but to design superior systems and interfaces for them. SchedCP embodies
this by providing the essential tools and safety guarantees
for any agent to interact with the Linux kernel’s scheduler,
analogous to how an environment in reinforcement learning
provides the state, actions, and rewards for an agent to learn.
This section details SchedCP’s design principles, core components, and implementation. SchedCP is implemented in
4000 lines of Rust and 6000 lines of python (include tests).
4.1

Design Principles

The design of SchedCP is governed by four key principles
that ensure it is safe, efficient, and future-proof.
Decoupling and Role Separation: A system tightly coupled to a specific AI model’s capabilities will quickly become
obsolete as models evolve. To ensure our framework is futureproof, we believe the system’s role must be separated from
the AI’s. Our principle is to decouple “what to optimize” (the
AI’s domain) from “how to observe and act” (the system’s
domain). We treat the AI agent as a performance engineer
using a stable set of tools provided by the system, allowing

Conference’17, July 2017, Washington, DC, USA

Yusheng Zheng, Yanpeng Hu, Wei Zhang, and Andi Quinn

the framework to remain relevant even as AI capabilities
advance.
Safety-First Interface Design: Autonomous agents with
kernel access pose inherent risks. System stability is nonnegotiable, so we treat AI as potentially non-cautious actors
and design defensive interfaces. The framework prevents
catastrophic failures by default rather than trusting agents
to avoid them.
Context and Feedback Balance: LLM agents face constraints from finite context windows and token costs. Performance degrades when flooded with irrelevant data. We
address this through adaptive context provisioning: agents
start with minimal summaries and progressively request
details as needed, balancing cost against precision.
Composable Tool Architecture: Rigid workflows stifle
LLMs’ ability to reason and devise novel solutions. Following Unix philosophy, we provide atomic tools and let agents
construct complex workflows through their reasoning capabilities, enabling novel solution generation.

and config, beginning with the kernel’s standard eBPF verifier to guarantee fundamental memory safety and termination. However, because the standard verifier is agnostic to
scheduling logic, it cannot detect flaws like task starvation
or unfairness; therefore, our pipeline adds a crucial second
layer of scheduler-specific static analysis checkers using customize verifier to check for these correctness and logic bugs.
Code that passes both static analysis layers proceeds to dynamic validation, where it is compiled and executed within a
secure micro-VM against correctness and performance tests.
Upon success, the service issues a signed deployment token
for a monitored canary deployment, which includes a circuit breaker to automatically revert to the last known-good
scheduler if performance degrades, ensuring all policies are
rigorously vetted before production use. It also ensures the
sched-agent doesn’t need root access to deploy eBPF schedulers.

4.2

Building on SchedCP, we developed sched-agent, a multiagent AI framework for scheduler optimization. At its core,
sched-agent implements in-context reinforcement learning
(ICRL)[24], a paradigm where the agent adapts its strategy
based on recent performance feedback in the context without costly model retraining. We realized this framework using Claude Code’s subagent architecture[5], which provides
specialized AI assistants with customized system prompts,
tools, and separate context windows[3]. Mirroring the collaboration of expert human teams, this multi-agent structure
naturally decomposes the complex optimization process into
the distinct stages of the ICRL loop: observation (state), planning/execution (action), and learning (reward analysis).
To automatically trigger optimization, SchedCP integrates
with container orchestrators and runtime like Kubernetes
and Docker, enabling it to initiate the sched-agent’s analysis
cycle whenever a user deploys a new application. It can also
be triggered manually by user.

Core Components and Implementation

SchedCP is engineered as a modular control plane, exposing its services to AI agents via the standard Model Context
Protocol (MCP) [2]. This design cleanly separates the highlevel policy orchestration managed by the agent from the
low-level observation and execution handled by the framework, and avoids granting ‘root’ privileges to the agent. The
architecture consists of three primary services.
1. Workload Analysis Engine. This service provides
agents with tiered access to system performance data. It
offers three levels of information: (1) cost-effective API endpoints delivering pre-processed summaries like CPU load
and memory usage, (2) secure sandbox access to basic file
reading, application building, standard Linux profiling tools
(perf, top) and dynamically attachable eBPF probes for detailed analysis, and (3) a feedback channel that reports postdeployment performance metrics such as percentage change
in makespan or latency. The service implements adaptive
context provisioning, allowing agents to request progressively detailed information as needed.
2. Scheduler Policy Repository. This service is a vector
database storing eBPF scheduler code with rich metadata:
natural language descriptions, target workloads, and historical performance metrics. It also includes a set of executable
scheduler programs. It provides APIs for semantic search
and retrieval, enabling agents to find relevant schedulers or
composable code primitives. To support system evolution,
it includes endpoints for updating performance metrics and
promoting new policies. The repository reduces generation
costs by allowing reuse of proven solutions while maintaining a growing library of scheduling strategies.
3. Execution Verifier. This validation pipeline service
provides multi-stage verification for all AI-generated code

5

5.1

sched-agent: A Multi-Agent Framework
for OS Optimization

Agent Roles and Responsibilities

5.1.1 Observation & Analysis Agent - Building a
Workload Profile. The Observation Agent builds a comprehensive “Workload Profile” by strategically querying the
Workload Analysis Engine. Its reasoning process determines
the analysis sequence: starting with high-level summaries,
then requesting deeper profiling based on initial findings. For
example, after identifying a parallel build process through
initial queries, the agent decides to request CPU statistics via
perf stat and top. Importantly, the agent does not require
the workload to be re-run; it can quickly adapt to new and
incoming workloads by continuously monitoring real-time
performance metrics like CPU utilization rates, memory access patterns, I/O throughput, and application-level profiling

Towards Agentic OS: An LLM Agent Framework for Linux Schedulers

data. This enables rapid response to changing workload characteristics without the overhead of repeated execution. The
agent synthesizes these data points into a description of the
workload in natural language, quantified performance characteristics, and explicit optimization goals. It manages the
cost-precision tradeoff by requesting only essential information and can register for event notifications in SchedCP to
trigger re-analysis when workload patterns change.
5.1.2 Planning Agent - Policy Synthesis and Selection.
The Planning Agent transforms the Workload Profile into
an optimization strategy. It constructs semantic queries for
the Scheduler Policy Repository based on the profile’s keywords and performance goals. The agent’s decision logic follows a hierarchy: search for exact matches, broaden to similar
patterns if needed, then decide among three pathways. For
existing production-ready scheduler solutions with strong
performance history, it configures parameters. For partial
matches, it retrieves code and generates patches. When no
suitable base exists, it composes new schedulers from algorithm primitives. The agent evaluates tradeoffs between
reuse efficiency and customization needs using historical
performance data from the repository.
5.1.3 Execution Agent - Validated Policy Deployment.
The Execution Agent manages the development, validation
and deployment process. It synthesizes code artifacts based
on the Planning Agent’s strategy, then submits them to the
Execution Verifier. The agent interprets validation results
and adapts accordingly: when static analysis fails, it refines
the code; when dynamic tests fail, it analyzes errors and fixes
logic issues. The agent decides whether to proceed, retry,
or abandon approaches based on verifier feedback. Upon
receiving a deployment token, it initiates canary rollout. If
the circuit breaker triggers, the agent captures failure context
and determines next steps, either revising the approach or
escalating to the Learning Agent.
5.1.4 Learning Agent - Performance Analysis and
Knowledge Update. The Learning Agent completes the
in-context reinforcement learning loop and analyzes deployment outcomes to improve future performance. It retrieves
metrics from the Feedback Channel and identifies success
patterns and failure modes. Crucially, the agent learns from
live performance data as the scheduler operates on actual incoming workloads, enabling continuous improvement without disrupting service. For immediate benefit, it informs
subsequent optimization cycles within the current session.
For long-term improvement, it updates the Scheduler Policy
Repository: refining performance metrics, annotating schedulers with deployment contexts, and promoting successful
novel policies. The agent documents antipatterns from failures to prevent repetition. This dual approach enables both
in-session adaptation and persistent system-wide learning.

Conference’17, July 2017, Washington, DC, USA

5.2

Example: Kernel Compilation

To illustrate how these four agents work together, consider
a kernel compilation workload. The Observation Agent
begins by analyzing the Linux kernel source tree, executing
make -j to understand the build process, and collecting resource usage like CPU, memory. This observation produces
a Workload Profile: “CPU-intensive parallel compilation task
with short-lived processes, inter-process dependencies, and
a goal to minimize makespan.” During planning, the Planning Agent queries the Scheduler Policy Repository with
keywords like “throughput” and “compilation,” retrieving
scx_rusty as a starting point. It generates a configuration to
make the scheduler more adaptive to the build process. In execution, the Execution Agent submits the patched code to
the Execution Verifier for validation, receiving a deployment
token upon success. Finally, after deployment, the Learning
Agent receives feedback that the revision achieved a 45%
reduction in makespan, contributing the improved scheduler back to the Scheduler Policy Repository for future use.
This entire workflow demonstrates how sched-agent enables
AI agents to autonomously optimize system performance
through iterative refinement.

6

Evaluation

We investigate key research questions to validate the effectiveness and efficiency of SchedCP:
• RQ1: Can SchedCP effectively configure existing
schedulers?
• RQ2: Can SchedCP generate new schedulers for specific workloads?
• RQ3: What is the cost and efficiency of SchedCP’s
scheduler generation?
• RQ4: How much can sched-agent continue to improve
performance after initial attempt?
6.1

Experimental Setup

We evaluate SchedCP on two machines, machine 1 is an 86core, 172 threads Intel Xeon 6787P with 758GB RAM, NVMe
SSDs, 10Gbps network, with 2x 256 GB CXL (Compute Express Link) memory device, 3 numa node, running Linux
6.14 with sched_ext. Machine 2 is an 8-core, 8 threads Intel
Core Ultra 7 258V with 30GB RAM, NVMe SSDs, 1 NUMA
node, running Linux 6.13 with sched_ext. We test Claude
Code (Opus 4) as AI agents to validate framework generality.
For each case, we test 3 times and get the average results.
To mitigate cache warming effects, we clear the page cache
(via sync; echo 3 > /proc/sys/vm/drop_caches) before
each run and perform a warm-up run that is excluded from
measurements. In all the experiments, the Agent successfully creates working custom scheduler configurations or
generates new eBPF programs.

Conference’17, July 2017, Washington, DC, USA

0

10

6.3

AI-Generated Schedulers for Batch Workloads

Figure 4 evaluates the SchedCP and sched-agent ability to
generate new schedulers from scratch (not merely select existing ones) on 8 diverse batch workloads (e.g. file compression, video transcoding, software testing, and data analytics

7.1

6.3

5

23.5

10.710.3

7.0

5.1

6.6

5.4

1.5 1.4

s
tic
aly
an

ct_

pro
du

al_

Test Cases

vir

vid

eo

_tr

an

reg

sco

ati

de

on

t
ren
ho

tke

y_a

gg

m
ksu

git

dd

_ad

ec

cte

res
s
mp
co

sis

0

ion

Figure 2. Performance comparison of scheduler configurations in Linux build benchmark.

11.0
8.8

aly

16

We further evaluate SchedCP on machine 2 using
schbench [22], a scheduler benchmark measuring wakeup
latency and throughput. Figure 3 compares three configurations: default EEVDF, initial selection (scx_bpfland), and
iterative optimization (scx_rusty after 3 iterations). While
AI configured scheduler initially underperformed with 13%
worse P99 latency (46.1ms vs 40.3ms) and 19% lower throughput (741 vs 910 req/s), AI iterative refinement identified
scx_rusty as superior. After three iterations, scx_rusty
achieved 2.11× better P99 latency (19.1ms) and 1.60× higher
throughput (1452 req/s) versus EEVDF, demonstrating our
agent’s effective learning from performance feedback.

1750

17.6

_ch

13.57s

14

1500

16.7

file

12

1250

15

ite

10

20

Average Build Time (seconds)

1000

Default Scheduler
Custom Scheduler

_an

8

750

Throughput (req/s) / P99 Latency (ms)

22.8

log

6

500

Throughput (req/s)
P99 Latency (ms)

910 req/s

tasks) running on machine 2. To simulate a long-tail distribution, each workload comprised 40 parallel tasks: 39 short and
one significantly longer, each as a python script or C/C++
program. The agent consistently identified this pattern and
generated custom eBPF code implementing a Longest Job
First (LJF) scheduling policy—a scheduler not present in our
repository—achieving an average 20% reduction in end-toend processing time. The cost for this analysis averaged $0.15
per workload, based on Claude Opus 4 pricing from August
2025. We note that the powerful Claude Opus agent successfully classified all 8 workloads, whereas the smaller Claude
Sonnet model could not. In addition to performance gains,
our framework’s optimizations reduced generation costs per
iteration: time fell from 33 to 2.5 minutes (a 13x reduction),
and the monetary cost dropped from $6 to $0.5.

st_
su

4

741 req/s (0.81×)

Figure 3. Schbench performance comparison showing P99 latency and throughput improvements through iterative scheduler optimization.

7.60s 1.79×
8.31s 1.63×
2

250

os_

0

1452 req/s (1.60×)

First Attempt 46.1 ms (0.87×)

Default Linux 40.3 ms

13.79s 0.98×

Scheduler

basic RL scheduler
Iter 3 times
first attempt
default Linux EEVDF

Iter 3 times 19.1 ms (2.11×)

d_d
iffe

We evaluate the SchedCP and sched-agent’s ability to both select/configure existing schedulers and generate new ones, as
well as learn after the first attempt. The iterative refinement
simulates realistic deployment scenarios where schedulers
are evaluated over multiple runs before production deployment. Note that the attempt counts measure the iteration of
the observe-optimization process in the AI Agents, it does
not require the workload to be re-run. The Linux kernel
build benchmark compiles the kernel 6.14 with tinyconfig
and “make -j 172” on machine 1. Figure 2 shows performance
improvements across three stages: baseline EEVDF, initial AIselected schedulers, and iteratively-refined configurations.
We also compare with pre-trained RL algorithms that have
been proposed for scheduler optimization [11] out of the box,
which only tunes scheduler parameter. The workload shows
1.63x speedup from 13.57s to 8.31s using scx_rusty as the
first attempt. After 3 iterations of observe-optimization process, the sched-agent selects the scx_layered scheduler and
adds 16% additional gain beyond LLM configuration, with
total improvements of 1.79x over baseline EEVDF. In contrast, basic RL approaches show no improvement in our tests,
likely because they require hardware or workload-specific
retraining, which is costly and time-consuming.

Scheduler

AI configured schedulers for kernel build and
schbench

Average Wall Clock Time (seconds)

6.2

Yusheng Zheng, Yanpeng Hu, Wei Zhang, and Andi Quinn

Figure 4. AI-generated scheduler performance on batch
workloads.

7

Related Work

Machine learning has a history of optimizing systems, including learned indexes [15], database tuning [21, 31], and RLbased job schedulers [19, 28, 35] supported by platforms like
Park [20]. However, these methods require extensive training, lack the semantic understanding to transfer knowledge
across diverse workloads, or need human specify high level
optimization goals. While recent work has applied LLMs to
system diagnostics [32] and code generation [33, 37], our

Towards Agentic OS: An LLM Agent Framework for Linux Schedulers

work is the first to use an autonomous agent to design, configure and generate kernel schedulers, and apply them for endto-end system optimization without human involvement. By
leveraging LLM Agent reasoning, tool usage with sched_ext
and eBPF, our framework uniquely bridges the semantic gap
between application needs and system policy.

8

Future Work

While SchedCP demonstrates the viability of AI-driven
scheduler optimization, extending our framework beyond
schedulers to cache policies, DVFS, network configuration,
and sysctl parameters presents immediate opportunities for
a unified OS optimization framework. Cross-component optimization, where CPU, memory, I/O, and power decisions inform each other, could unlock significant performance gains
through new abstractions for expressing inter-component
dependencies. This work opens new possibilities for adaptive,
application-aware operating systems that can automatically
optimize themselves for changing workloads, making expertlevel performance accessible to all users.

9

Conclusion

We introduce SchedCP, the first framework for autonomous
LLM agents to safely optimize Linux schedulers. Its decoupled control plane separates AI reasoning from safe system
execution, bridging the gap between application needs and
kernel policy. Our agent, sched-agent, achieved up to a 1.79×
speedup and a 13x cost reduction over naive approaches
by autonomously generating and deploying custom eBPF
scheduling policies while ensuring stability. SchedCP offers a stable interface for AI-driven optimization, marking
a foundational step towards truly application-aware, selfoptimizing operating systems.

References
[1] Anthropic. 2024. The Claude 3 Model Family: Opus, Sonnet, Haiku.
Anthropic Technical Report (2024).
[2] Anthropic. 2024. Model Context Protocol. https://www.anthropic.
com/news/model-context-protocol. An open standard for connecting
AI assistants to data sources.
[3] Anthropic. 2025. How we built our multi-agent research system.
https://www.anthropic.com/engineering/built-multi-agentresearch-system. Engineering blog post on multi-agent systems.
[4] Anthropic. 2025. Introducing Claude Code. https://www.anthropic.
com/news/claude-code. Agentic coding tool announcement, Anthropic blog.
[5] Anthropic. 2025. Subagents - Claude Code Documentation. https://
docs.anthropic.com/en/docs/claude-code/sub-agents. Documentation
for Claude Code subagent implementation.
[6] Anysphere Inc. 2025. Cursor: The AI powered Code Editor. https:
//cursor.com/. AI assisted IDE with agent mode; latest release version
1.0 on June 4, 2025.
[7] The Cilium Authors. 2025. Cilium - Cloud Native, eBPF-based Networking, Observability, and Security. https://cilium.io/
[8] Fraol Batole, David O’Brien, Tien N. Nguyen, Robert Dyer, and Hridesh
Rajan. 2025. An LLM-Based Agent-Oriented Approach for Automated

Conference’17, July 2017, Washington, DC, USA
Code Design Issue Localization. In Proceedings of the 47th International
Conference on Software Engineering (ICSE).
[9] Harrison Chase. 2023. LangChain: Building applications with LLMs
through composability. https://github.com/langchain-ai/langchain
[10] Linux Kernel Community. 2024. sched_ext: BPF Scheduler Class. Linux
Kernel Documentation (2024). https://github.com/sched-ext/scx
[11] Jonathan Corbet. 2025. Improved load balancing with machine learning. https://lwn.net/Articles/1027096/. LWN.net (1 July 2025). Machine
learning approaches to Linux kernel scheduling.
[12] Mario De Jesus, Perfect Sylvester, William Clifford, Aaron Perez, and
Palden Lama. 2025. LLM-Based Multi-Agent Framework For Troubleshooting Distributed Systems. (2025).
[13] eBPF Community. 2023. eBPF Documentation. https://ebpf.io/
[14] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, et al. 2023.
MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. arXiv preprint arXiv:2308.00352 (2023).
[15] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis.
2018. The Case for Learned Index Structures. In Proceedings of the 2018
International Conference on Management of Data (SIGMOD). 489–504.
[16] Feng Lin, Dong Jae Kim, and Tse-Hsun Chen. 2025. SOEN-101: Code
Generation by Emulating Software Process Models Using Large Language Model Agents. In Proceedings of the 47th International Conference
on Software Engineering (ICSE).
[17] Linux Kernel Documentation. 2024. EEVDF Scheduler - Earliest Eligible Virtual Deadline First. https://docs.kernel.org/scheduler/schedeevdf.html. Introduced in Linux kernel 6.6, replacing CFS.
[18] Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen,
Lingming Zhang, and Yiling Lou. 2024. Large Language ModelBased Agents for Software Engineering: A Survey. arXiv preprint
arXiv:2409.02977 (2024). Survey of 106 papers on LLM-based agents
for SE. GitHub: FudanSELab/Agent4SE-Paper-List.
[19] Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan,
Zili Meng, and Mohammad Alizadeh. 2019. Learning Scheduling
Algorithms for Data Processing Clusters. In Proceedings of the ACM
Special Interest Group on Data Communication (SIGCOMM). 270–288.
[20] Hongzi Mao, Shaileshh Bojja Venkatakrishnan, Malte Schwarzkopf,
and Mohammad Alizadeh. 2019. Park: An Open Platform for LearningAugmented Computer Systems. In Advances in Neural Information
Processing Systems (NeurIPS).
[21] Ryan Marcus and Olga Papaemmanouil. 2019. Neo: A Learned Query
Optimizer. In Proceedings of the VLDB Endowment, Vol. 12. 1705–1718.
[22] Chris Mason. 2016. schbench: Scheduler Benchmark. https://kernel.
googlesource.com/pub/scm/linux/kernel/git/mason/schbench.
A
scheduler benchmark that measures wakeup latency and throughput.
[23] microsoft. 2024. eBPF for Windows. https://github.com/microsoft/
ebpf-for-windows.
[24] Amir Moeini, Jiuqi Wang, Jacob Beck, Ethan Blaser, Shimon Whiteson,
Rohan Chandra, and Shangtong Zhang. 2025. A Survey of In-Context
Reinforcement Learning. arXiv preprint arXiv:2502.07978 (2025). https:
//arxiv.org/abs/2502.07978 Version 1, February 11.
[25] Taylor Mullen and Ryan J. Salva. 2025. Gemini CLI: Your Open Source
AI Agent. https://blog.google/technology/developers/introducinggemini-cli-open-source-ai-agent/. Google Developers Blog, Jun 2025.
[26] OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774
(2023).
[27] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, et al. 2024. ChatDev:
Communicative Agents for Software Development. In Proc. ACL.
[28] Haoran Qiu, Siddhartha Banerjee, Saurabh Jha, Shivaram Kalyanaraman, and Chuan Tang. 2020. FIRM: An Intelligent Fine-grained Resource Management Framework for SLO-Oriented Microservices. In
14th USENIX Symposium on Operating Systems Design and Implementation (OSDI). 805–825.

Conference’17, July 2017, Washington, DC, USA
[29] Aqua Security. 2023. Tracee: Runtime Security and Forensics using
eBPF. https://github.com/aquasecurity/tracee
[30] Ramneet Singh, Sathvik Joel, Abhav Mehrotra, Nalin Wadhwa, Ramakrishna B Bairi, Aditya Kanade, and Nagarajan Natarajan. 2025.
Code Researcher: Deep Research Agent for Large Systems Code and
Commit History. arXiv preprint arXiv:2506.11060 (2025). Microsoft
Research.
[31] Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, and Bohan Zhang.
2017. Automatic Database Management System Tuning Through
Large-scale Machine Learning. In Proceedings of the 2017 ACM International Conference on Management of Data (SIGMOD). 1009–1024.
[32] Hao Wang et al. 2024. LLMs for System Understanding and Optimization. arXiv preprint (2024).
[33] Xin Wei et al. 2024. Automatic Parallel Program Mapper Generation
with LLMs. In Proceedings of ASPLOS.
[34] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023.
AutoGen: Enable Next-Gen Large Language Model Applications. https:

Yusheng Zheng, Yanpeng Hu, Wei Zhang, and Andi Quinn
//github.com/microsoft/autogen
[35] Wei Zhang et al. 2024. Multi-Resource Scheduling with Reinforcement
Learning. IEEE Transactions on Parallel and Distributed Systems (2024).
[36] Yusheng Zheng, Yiwei Yang, Maolin Chen, and Andrew Quinn. 2024.
Kgent: Kernel Extensions Large Language Model Agent. In Proceedings
of the ACM SIGCOMM 2024 Workshop on EBPF and Kernel Extensions
(Sydney, NSW, Australia) (eBPF ’24). Association for Computing Machinery, New York, NY, USA, 30–36. doi:10.1145/3672197.3673434
[37] Yusheng Zheng, Yiwei Yang, Maolin Chen, and Andrew Quinn. 2024.
Kgent: Kernel Extensions Large Language Model Agent. In Proceedings
of the ACM SIGCOMM 2024 Workshop on EBPF and Kernel Extensions
(Sydney, NSW, Australia) (eBPF ’24). Association for Computing Machinery, New York, NY, USA, 30–36. doi:10.1145/3672197.3673434
[38] Yusheng Zheng, Tong Yu, Yiwei Yang, Yanpeng Hu, Xiaozheng Lai, Dan
Williams, and Andi Quinn. 2025. Extending Applications Safely and
Efficiently. In 19th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 25). 557–574.

