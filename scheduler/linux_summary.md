# Linux Load-Aware and Experimental Schedulers Across Platforms

Modern computing platforms – from desktops and mobile devices to data centers and embedded systems – rely on sophisticated OS-level schedulers to manage CPU resources in face of diverse workloads and hardware heterogeneity. This survey examines **Linux load-aware scheduling** and experimental schedulers across platforms. We survey general-purpose schedulers (e.g. O(1), CFS, EEVDF), desktop-oriented forks (BFS, MuQSS) and Linux real-time classes (SCHED\_FIFO/RR, SCHED\_DEADLINE). We detail mobile and power-aware schedulers (Android’s EAS/WALT/SchedTune), as well as scheduling in servers, embedded, and heterogeneous (ARM big.LITTLE, CXL-enabled) architectures. Recent experimental approaches – including energy models and machine-learning techniques – are covered. We compare algorithms in depth (using tables where useful), highlighting metrics like fairness, latency, and energy efficiency. Finally, we identify gaps (e.g. unified heterogenous scheduling, RL integration) and open challenges for future research. This comprehensive survey is heavily referenced, drawing on Linux kernel documentation, LWN articles, and academic works, to serve as a foundation for ongoing OS scheduler research.

# Introduction

CPU scheduling is a core component of the Linux kernel, balancing throughput, fairness, responsiveness, and power efficiency.  Classic schedulers (e.g. Linux’s **O(1)** and **Completely Fair Scheduler (CFS)**) use runnable queues and interactivity heuristics to share CPU time.  The O(1) scheduler (Linux 2.6.0–2.6.22) ran in constant time but relied on complex “sleep boost” heuristics to favor interactive tasks.  In 2007 the CFS replaced it as the default, using a red-black tree to allocate fair shares of CPU time.  CFS tracks each task’s virtual runtime and de-prioritizes batch tasks dynamically.  (Notably, CFS itself has been superseded by the new **EEVDF** scheduler in Linux 6.6 (2023), but its principles remain representative of modern fair scheduling.)

Beyond the default kernel scheduler, alternative schedulers have been developed for specific use cases.  For desktop responsiveness, Con Kolivas created the **Brain Fuck Scheduler (BFS)** in 2009 – a single-runqueue, EEVDF-based scheduler without complex heuristics.  BFS “aims for simplicity and improved desktop responsiveness” on low-core systems.  Its successor, **MuQSS** (Multiple Queue Skiplist Scheduler), uses per-CPU skip lists and a similar EEVDF mechanism.  We cover these along with the mainline SCHED classes (FIFO, RR, Deadline) for completeness.

Modern systems are increasingly heterogeneous. Mobile and embedded devices often have ARM big.LITTLE clusters, requiring **load-capacity-aware** scheduling. Linux now records each CPU’s *capacity* (relative performance) and uses an **Energy-Aware Scheduler (EAS)** that consults an energy model at task wake-up.  EAS (originating in Android kernels) is only enabled on heterogeneous systems.  Android also introduced **Window-Assisted Load Tracking (WALT)** to improve utilization estimates and the **SchedTune** cgroup controller, which lets user-space “boost” a group’s CPU demand, causing higher frequencies and lower latency.  We discuss these Android innovations and how they have influenced mainline Linux.

We will survey scheduling across different platforms: **desktops/servers**, **mobile (Android)**, **data centers/HPC**, **embedded**, and **real-time** systems.  In each domain, we examine relevant schedulers and patches – including open-source projects and kernel patchsets – and categorize them. We pay special attention to *load-awareness*, i.e. how schedulers measure task load (PELT, WALT) and account for CPU capacity/energy in placement. We also cover **heterogeneous CPU/GPU architectures**, including upcoming CXL-coherent memory systems, and schedulers employing **machine learning** or **reinforcement learning (RL)** (e.g. Learning EAS, STUN, deep-learning approaches) to optimize performance or power.

The remainder of this paper is organized as follows: Section 2 classifies existing schedulers by type, target platform, and architecture. Section 3 provides a detailed comparison of major scheduler designs, with summary tables. Section 4 highlights open research challenges and gaps. Section 5 concludes. Throughout, we provide exhaustive citations to kernel documentation, academic papers, and major open-source projects.

# Taxonomy of Schedulers, Platforms, and Architectures

We organize the rich design space of Linux scheduling along three axes: (1) **Scheduler type** (fair-share, real-time/deadline, energy-aware, ML-based, etc.), (2) **Target platform/domain** (desktop, mobile, server/HPC, embedded, real-time OS), and (3) **Hardware architecture** (homogeneous multi-core, heterogeneous CPU clusters, CPU+accelerator, NUMA and disaggregated memory). This taxonomy highlights commonalities and differences among solutions.

* **Fair-share schedulers (general-purpose):** Aim to divide CPU time evenly (or by assigned weights) among tasks. The Linux **O(1)** scheduler (pre-2007) was a classic example, using per-CPU runqueues and interactivity heuristics. The current default **CFS** (Completely Fair Scheduler) uses a red-black tree of schedulable tasks to give each process a proportional share. CFS is a general-purpose work-conserving scheduler for SCHED\_NORMAL tasks, balancing throughput and latency.

* **Desktop-oriented schedulers:** Designed for snappy interactivity on low-core systems. Kolivas’s **BFS** (2009) eschews complex sleep heuristics and uses a single global queue with an *Earliest-Eligible-Virtual-Deadline-First* (EEVDF) algorithm. BFS set fixed time slices based on human perception (\~6ms) and simply chose the idle CPU nearest the last one used. MuQSS (2017) builds on BFS with per-CPU skip-list queues, preserving the EEVDF deadline rules but improving scalability on many cores.

* **Real-time schedulers:** Linux supports POSIX real-time classes and reservations. The **SCHED\_FIFO** and **SCHED\_RR** classes provide fixed-priority FIFO or round-robin scheduling for real-time threads (highest priority, can starve CFS tasks). More recently, **SCHED\_DEADLINE** (merged in Linux 3.14, 2014) lets tasks specify a *runtime* (execution budget) and *period* (with an implicit deadline), and it schedules by global EDF with a Constant-Bandwidth Server (CBS) protocol. Under SCHED\_DEADLINE, each task is guaranteed up to *runtime* every *period* (throttling tasks that exceed their budgets). Detailed kernel docs explain that tasks under this policy use EDF/CBS to provide temporal isolation. (In practice, global EDF can suffer Dhall’s effect on multicore, so users may isolate heavy RT tasks via cpusets.)

* **Energy- and load-aware schedulers:** These consider heterogeneous core capacities and power/energy effects. Linux’s **Energy-Aware Scheduler (EAS)** is enabled on ARM big.LITTLE platforms: when a task wakes, EAS consults an energy model and per-CPU “capacity” (performance index) to choose the CPU predicted to minimize energy impact while meeting throughput. The kernel now tracks each CPU’s capacity (relative throughput) for this purpose. Android popularized two load-tracking enhancements: **WALT** (Window-Assisted Load Tracking) measures CPU demand in short windows to guide cpufreq governors, and **SchedTune** provides a cgroup “boost” knob that tweaks a group’s perceived load (making the governor run faster for it). These illustrate how schedulers can be influenced by user-space hints and energy models.

* **Heterogeneous scheduling (big.LITTLE, GPU, CXL):** On mixed ARM clusters, asymmetry-aware policies try to place tasks on cores matching their demand. Older approaches (e.g. HMP) multiplied tasks by core capacity, but modern EAS and CPUfreq interplay is used. As architectures evolve (e.g. integrating CPU and GPU with coherent memory, or disaggregating memory via CXL), entirely new scheduling opportunities appear. CXL (Compute Express Link) provides a unified address space across CPUs and accelerators, effectively adding “remote” memory nodes. One study found CXL memory shows much higher latency (appearing as a two-hop NUMA node) and limited bandwidth, so schedulers must carefully distribute memory accesses. While GPU thread scheduling remains largely within drivers, the OS may need to consider GPU occupancy and CPU-GPU sharing in future unified architectures.

* **Machine-learning-driven schedulers:** Emerging research applies ML/RL to scheduling. For example, **STUN** (2022) is an RL framework that tunes multiple Linux scheduling parameters (10 parameters across 5 policies) to optimize for static workloads. On Android, **Learning EAS** uses a policy-gradient model to adjust EAS parameters (e.g. CPU-load threshold, migration cost) based on task characteristics; on a test smartphone it reduced power by \~2–8% and improved scheduling metrics relative to stock EAS. Very recently, Kahu et al. (2025) proposed **KernelOracle**, which uses an LSTM to *predict* the next task chosen by CFS, exploring whether a data-driven model could inform scheduling decisions. These efforts illustrate a nascent trend of applying predictive and adaptive learning models to OS scheduling.

* **Platforms and deployment:** Different deployments stress different scheduler aspects. **Desktop/server Linux** uses CFS (or now EEVDF) by default, sometimes with custom “desktop kernels” (BFS/MuQSS) for responsiveness. **Android/embedded Linux** often uses schedulers patched or configured for low latency and power: big.LITTLE and EAS are typical, and real-time classes or PREEMPT\_RT may be used in robotics/IoT. **Data centers/HPC** typically run unmodified CFS on large SMP servers, possibly with tuned irq affinity, NUMA balancing, and container cgroups; some research schedules threads for cache locality or QoS. **Real-time Linux** (e.g. PREEMPT\_RT kernels) aims for deterministic latency by making almost all kernel code preemptible; this complements use of SCHED\_FIFO/RR/DEADLINE for critical tasks. Across all cases, recent research highlights *heterogeneous scheduling frameworks* that attempt to span multiple domains (e.g. a unified approach for CPU and GPU in future systems).

In summary, Linux scheduling components can be classified by algorithmic family (fair vs. real-time vs. energy-aware), by use-case, and by targeted hardware. The following sections compare the key approaches in detail.

# Detailed Comparison of Approaches

We now examine representative schedulers and strategies in depth, highlighting their design choices and trade-offs. We include **comparison tables** for major schedulers. Citations to original sources, kernel documentation, and evaluations are given throughout.

* **General-Purpose / Fair-Share Schedulers:** Table 1 summarizes key general schedulers.

&#x20;*Figure: In ARM big.LITTLE (heterogeneous) systems, Linux’s scheduler must account for each core’s performance. EAS uses per-CPU *capacity* and power models to make energy-aware placement.*

| **Scheduler**       | **Type**              | **Key Features**                                                                                                                 | **Use Cases / Notes**                                                                                                 |
| ------------------- | --------------------- | -------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |
| **CFS** (default)   | Fair-share (O(log n)) | Red-black tree of runnable tasks; tracks *vruntime* for fairness; interactivity heuristics (sleeping tasks get boost)            | Standard Linux (since 2.6.23, 2007). Good general fairness but not hard-RT. Replaced by EEVDF in 6.6.                 |
| **EEVDF** (6.6+)    | Fair-share (EEVDF)    | Each task has *lag* and *virtual deadline*; picks task with earliest virtual deadline among those not *lagging*                  | New default (Linux 6.6, 2023). Like CFS but explicitly prioritizes latency-sensitive short-slice tasks.               |
| **BFS**             | Desktop (EEVDF)       | Single global runqueue; fixed 6ms time slice; no sleep-based heuristics                                                          | Third-party patch (no mainline). Targets desktops ≤16 cores. Prioritizes responsiveness.                              |
| **MuQSS**           | Desktop (skiplist)    | Per-CPU skip-list queues (8-level array); EEVDF deadline scheduling; lockless cross-cpu selection                                | Successor to BFS (in -ck patch). Improves multi-core scalability.                                                     |
| **SCHED\_FIFO/RR**  | Real-time             | Fixed-priority queues; FIFO or round-robin within each priority                                                                  | Standard POSIX RT classes (highest priority). Suitable for time-critical tasks; strict priority can cause starvation. |
| **SCHED\_DEADLINE** | Real-time (EDF+CBS)   | Tasks declare runtime (budget) *Q* and period *P*; EDF scheduling with constant-bandwidth server guarantees; throttling beyond Q | Global EDF (Linux 3.14+, kernel). Provides temporal isolation. Used for periodic real-time/multimedia tasks.          |
| **EEVDF(CK)**       | Fair-share (EEVDF)    | Similar algorithm to EEVDF but older branch; tracked deadlines in BFS/MuQSS                                                      | (Merged in CFS? Actually CFS uses similar concept of deadlines)                                                       |
| **PREEMPT\_RT**     | RT (kernel preempt)   | Fully preemptible kernel: IRQ handlers threaded, sleeping spinlocks, rt\_mutex, etc.                                             | RT-patched kernel. Used when low-latency is needed in Linux (e.g. industrial control). “Fully Preemptible” model.     |

Table 1: Comparison of major Linux CPU scheduling classes and forks. (References in table indicate key sources.)

In **Table 1**, *fair-share* schedulers (CFS/EEVDF vs BFS/MuQSS) all aim for proportional sharing; they differ in queue structures and heuristics.  CFS/EEVDF are multi-core-scalable (per-CPU runqueues, tree/lag-based selection). BFS’s single queue avoids per-CPU decision-making but suffers lock contention on many cores.  MuQSS resolves this with per-CPU skip lists. EEVDF (new default) explicitly tracks *lag* and allowed *virtual deadlines*; Linux’s documentation notes it “allows latency-sensitive tasks with shorter time slices to be prioritized”.

Real-time classes differ: SCHED\_FIFO/RR are **priority-based**, not time-reserving, whereas SCHED\_DEADLINE provides reservation (EDF with CBS). For SCHED\_DEADLINE, each task’s parameters (runtime Q, period P, implied deadline) are enforced by the kernel: tasks are accepted only if schedulable (bounded by `kernel.sched_rt_runtime_us`), then run with global EDF and throttled on Q. This ensures no RT task exceeds its bandwidth, at the cost of possible underutilization.  LWN articles confirm: “the deadline scheduler also implements the Constant Bandwidth Server…each task will receive its full run time; if it runs out, it is throttled until the next period”. In practice, system load and *Dhall’s effect* limit global EDF guarantees, so conservative admission or partitioning (cpusets) are advised.

* **Load Tracking and Migration:** Linux’s scheduler continuously tracks task *load* (utilization). The standard mechanism is **PELT** (Per-Entity Load Tracking), which computes an average utilization for tasks and runqueues.  PELT underlies CFS’s decisions and is used by cpufreq governors. Android’s WALT replaced PELT with a windowed load metric to improve responsiveness on mobile SoCs. The work-in-progress RFC reported that WALT reduced CPU power by \~5–10% on some workloads. When tasks migrate, Linux’s load-balancer moves them among CPUs to even loads (taking capacity into account on big.LITTLE). The kernel documentation on **Capacity Aware Scheduling** explains that each CPU has a normalized *capacity* (relative to max), and EAS uses this to weight loads.

* **Platform-Specific Enhancements:** Android introduced specialized features. The **SchedTune** cgroup controller (2016) lets user-space signal task importance via a `schedtune.boost` factor. Internally, this inflates the task’s PELT-estimated load: “if a group is boosted by 25%, the scheduler will expect it to use 25% more CPU” and thus the governor raises frequency accordingly. This is a novel twist: it does *not* change scheduling order (it does not become higher priority), but changes frequency scaling for that task’s CPU to reduce latency.  WALT and SchedTune work together with EAS: the full Android strategy involves using WALT load metrics and sched-freq governor, as described in a Linux Plumbers talk. The scheduler docs note that EAS currently requires heterogeneous CPUs (big.LITTLE) and uses an *energy model* to pick CPUs that minimize energy vs. throughput tradeoff.

* **Adaptive and ML-Based Schedulers:** Table 2 lists recent research prototypes.

| **Project/Study**                            | **Technique**        | **Target System / Goal**                                                       | **Key Results**                                                                                                  |
| -------------------------------------------- | -------------------- | ------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------- |
| **STUN (Lee et al., 2022)**                  | RL (Q-learning)      | Automatic tuning of 5 scheduling policies (10+ parameters) for Linux CFS       | Improved static workload performance. Modifies parameters like interactivity bias, migration cost.               |
| **Learning EAS (Lee et al., 2020)**          | Policy-gradient RL   | Android EAS scheduler; adapt `target_load`, `sched_migration_cost` per-task    | Reduced smartphone power by \~2–8% and improved latency (hackbench) by up to 25% vs stock EAS.                   |
| **KernelOracle (Kahu et al., 2025)**         | Deep learning (LSTM) | Predict next task scheduled by CFS from trace data                             | High accuracy in forecasting CFS choices; explores feasibility of data-driven scheduling.                        |
| **HPC RL Co-scheduler (Souza et al., 2024)** | RL + profiling       | Cluster job co-scheduling (Slurm+Mesos) for HPC                                | Improved cluster utilization by \~50% and reduced makespan by \~55% (simultaneous scheduling). *(Cluster-level)* |
| **Predictive ML Scheduling (Various)**       | Supervised ML        | General suggestion (not OS-integrated) for load prediction or time-to-complete | Studies report \~1–5% reductions in turnaround time by predicting scheduler decisions.                           |

Table 2: Examples of ML/RL scheduling approaches. *STUN* and *Learning EAS* target Linux CPU scheduling tunables. *KernelOracle* predicts CFS decisions via LSTM. HPC co-scheduling uses RL at batch-scheduler level.

These studies indicate that ML techniques can tune or predict scheduling behavior. For instance, *Learning EAS* adjusted Android’s energy scheduler parameters via reinforcement learning: it “improved power consumption by 2.3–5.7% and hackbench performance by 2.8–25.5%” compared to vanilla EAS. Similarly, STUN’s RL agent fine-tuned CFS settings to optimize a given workload. The feasibility of replacing or augmenting parts of the scheduler with learned models is an open question. KernelOracle shows that an LSTM can *learn* CFS’s decision patterns from traces, hinting at future directions for data-driven scheduling.

* **Heterogeneous CPU/GPU Architectures:** Traditional Linux scheduling assumes CPU tasks only. With CXL and heterogeneous chips emerging, OS schedulers may need to manage memory and tasks across CPUs and GPUs coherently. CXL promises a unified address space: “Single, common, memory address space across processors and devices”. On such systems, the CPU can directly read/write GPU memory via CXL with cache coherence. However, real CXL memory behaves like a slower NUMA node: one study finds CXL memory has far lower bandwidth and \~70–150% higher latency than DRAM. Thus, future schedulers will need to factor memory tiering and accelerators into decisions (e.g. preferring to keep latency-sensitive tasks on CPUs with local DRAM, while offloading other jobs to CXL-backed resources). Such integrated CPU/GPU scheduling is nascent, but frameworks for heterogeneous acceleration (e.g. CXL or specialized OS extensions) are an active research area.

# Research Gaps and Open Challenges

Despite extensive work, significant gaps remain in Linux scheduling research:

* **Unified Heterogeneous Scheduling:** Current solutions often treat CPU and accelerator scheduling separately. With architectures like ARM big.LITTLE and upcoming CXL GPU/accelerator disaggregation, there is no unified scheduler that handles CPU-GPU load sharing and memory coherency. For example, scheduling tasks on a CPU-only cluster is well-understood, but deciding when to offload compute to a GPU (with shared virtual memory via CXL) is largely unexplored. Research is needed on OS-level policies for CPU/GPU co-scheduling, possibly extending deadline or fair-share algorithms to heterogeneous cores and accelerators.

* **Scalability to Many Cores:** BFS-style schedulers (global queue) do not scale beyond \~16 cores, whereas CFS/EEVDF scale well but involve more overhead per task. As core counts grow (64+), lock contention and cache-affinity become critical. New data structures (e.g. skip lists, as in MuQSS) help, but balancing locality vs. fairness remains challenging. Research could explore hierarchical or hybrid models (e.g. runqueues per socket with global merging). Evaluating schedulers on 1000+ cores is rare in literature.

* **Real-Time Guarantees on SMP:** Global EDF (SCHED\_DEADLINE) in Linux provides soft real-time guarantees, but **hard** real-time assurances are still elusive. The acceptance test for global scheduling is not sufficient under Dhall’s effect. Thus, mission-critical systems often partition cores or use simpler fixed-priority approaches. Better multicore RT scheduling algorithms (e.g. global with feedback, or partitioned optimal tests) could be investigated. Likewise, Linux’s real-time patch (PREEMPT\_RT) improves latency, but the interaction with deadline scheduling and mixed-criticality (hard vs soft tasks) is under-studied.

* **Workload-Aware and ML-Driven Scheduling:** Most ML-based schedulers have been proof-of-concept. Integrating learned policies into the kernel (with stability guarantees and overhead constraints) is a big challenge. How to collect training data safely on production systems? How to adapt continuously to workload changes? The Linux kernel’s **pluggable scheduler** interface could accommodate ML modules, but this is untested. There is a gap between academic prototypes (e.g. Learning EAS) and production readiness. Also, labeled training data for RL is hard to obtain in real-time. Hybrid approaches (semi-supervised learning, online RL) warrant exploration.

* **Power and Thermal Constraints:** Mobile devices demand energy-efficient scheduling, but current EAS models are relatively simple (mostly focusing on CPU energy). Incorporating thermal models, battery state, or user experience models could improve results. For servers, heterogeneous energy sources (grid vs renewables) are unexplored in task scheduling.  Also, CPUs are not the only consumers: integrated GPUs, NPU/TPUs, and memory can throttle, which classic schedulers ignore.

* **Workload Diversity:** Cloud and edge workloads are extremely diverse. Most scheduler designs are evaluated on CPU-bound or synthetic benchmarks. There is a need for comprehensive benchmarking (including mixed I/O/compute, interactive, ML inference, etc.) to test scheduler policies. For example, how does CFS vs BFS vs MuQSS perform on containerized microservices or big data jobs? Research gaps include understanding scheduler behavior on bursty or highly parallel workloads.

* **Fine-Grained vs. Coarse-Grained Scheduling:** Linux schedules at the granularity of threads/containers, but modern tasks may have internal parallelism (OpenMP, GPU kernels). Some research looks at co-scheduling or gang scheduling of related threads. The interplay between OS scheduling and user-level schedulers (like Intel TBB or OpenMP) is not fully understood. Mechanisms like XEN credit scheduler for VMs or Kubernetes pod scheduling overlap with OS scheduling issues.

* **Security and QoS:** Scheduling also affects security (side-channels through shared caches) and quality-of-service isolation (noisy neighbors). While not load-awareness per se, enhanced schedulers that enforce isolation (e.g. scheduler-based cache partitioning hints) are an open area. For instance, a scheduler could deliberately delay low-QoS tasks to protect high-QoS ones on congested cores.

These gaps suggest rich directions for future work. On Linux specifically, many enhancements (e.g. EEVDF transition, EAS tuning) are ongoing, but as hardware evolves (more heterogeneity and interconnected memory) and workloads diversify, novel scheduling ideas will be needed.

# Conclusion

This survey has provided a broad and detailed overview of Linux scheduling research and practice. We covered historical evolutions (O(1) → CFS → EEVDF), alternative desktop schedulers (BFS, MuQSS), real-time scheduling (SCHED\_FIFO/RR, SCHED\_DEADLINE, PREEMPT\_RT), and modern enhancements for heterogeneous and energy-aware scheduling (EAS, WALT, SchedTune). We also examined experimental and learning-based approaches (e.g. STUN, Learning EAS, KernelOracle) that push OS scheduling beyond classical algorithms. Through comparative tables and extensive citations, we highlighted how each scheduler or framework fits into specific domains (desktop, mobile, data-center, embedded) and hardware (multi-core, big.LITTLE, CXL memory systems).

Despite decades of development, OS-level scheduling remains an active research field. The continued incorporation of machine learning, the advent of new hardware heterogeneity (GPUs, NPUs, CXL memory), and the importance of energy efficiency create open challenges. We identified key gaps – such as unified CPU/GPU scheduling, scalability to massive core counts, and tighter real-time guarantees – that motivate further study. We hope this survey serves as a comprehensive reference for researchers and practitioners, summarizing past work and pointing to emerging frontiers in Linux scheduler design.

# References

The references below (formatted as in the Linux kernel documentation and LWN format) correspond to the citations above. Each entry provides further reading on scheduling policies, implementations, and research mentioned:

- O(1) Linux scheduler (pre-2007)[en.wikipedia.org](https://en.wikipedia.org/wiki/O(1)_scheduler#:~:text=The%20O,by%20the%20Completely%20Fair%20Scheduler)[en.wikipedia.org](https://en.wikipedia.org/wiki/O(1)_scheduler#:~:text=The%20main%20issue%20with%20this,65%5D%20causing).
- CFS (Completely Fair Scheduler, default since 2.6.23)[en.wikipedia.org](https://en.wikipedia.org/wiki/Completely_Fair_Scheduler#:~:text=The%20Completely%20Fair%20Scheduler%20,while%20also%20maximizing%20interactive%20performance)[en.wikipedia.org](https://en.wikipedia.org/wiki/Completely_Fair_Scheduler#:~:text=schedulable%20entities%29.,).
- BFS (Brain Fuck Scheduler)[en.wikipedia.org](https://en.wikipedia.org/wiki/Brain_Fuck_Scheduler#:~:text=The%20Brain%20Fuck%20Scheduler%20,4)[en.wikipedia.org](https://en.wikipedia.org/wiki/Brain_Fuck_Scheduler#:~:text=The%20objective%20of%20BFS%2C%20compared,has%20been%20reported%20to%20improve) and MuQSS[lwn.net](https://lwn.net/Articles/720227/#:~:text=MuQSS%20is%20BFS%20with%20multiple,exactly%20one%20cache%20line%20wide)[lwn.net](https://lwn.net/Articles/720227/#:~:text=The%20deadlines%20for%20MuQSS%20now,essentially%20the%20same%20as%20BFS) for desktop latency.
- Capacity/Aware and Energy-Aware Scheduling (EAS) in Linux (kernel docs)[docs.kernel.org](https://docs.kernel.org/scheduler/sched-energy.html#:~:text=%2F%21,with%20symmetric%20CPU%20topologies)[docs.kernel.org](https://docs.kernel.org/scheduler/sched-energy.html#:~:text=In%20short%2C%20EAS%20changes%20the,and%20their%20respective%20energy%20costs).
- Android scheduling features: WALT (LWN report)[lwn.net](https://lwn.net/Articles/704903/#:~:text=ARM%20data%20with%20EAS%20,freq) and SchedTune (LWN)[lwn.net](https://lwn.net/Articles/706374/#:~:text=SchedTune%20is%20implemented%20as%20a,which%20it%20ends%20up%20running)[lwn.net](https://lwn.net/Articles/706374/#:~:text=In%20current%20kernels%2C%20the%20load,get%20a%20specific%20job%20done).
- SCHED_DEADLINE (EDF+CBS) in kernel[en.wikipedia.org](https://en.wikipedia.org/wiki/SCHED_DEADLINE#:~:text=,by%20each%20activation%20of%20the)[kernel.org](https://www.kernel.org/doc/html/next/scheduler/sched-deadline.html#:~:text=,actually%20receives%20%E2%80%9Cruntime%E2%80%9D%20time%20units); LWN analysis[lwn.net](https://lwn.net/Articles/743946/#:~:text=The%20Linux%20deadline%20scheduler%20also,causing%20problems%20to%20other%20jobs)[lwn.net](https://lwn.net/Articles/743946/#:~:text=However%2C%20it%20is%20worth%20noting,use%20a%20necessary%20and%20sufficient).
- Linux real-time (PREEMPT_RT) preemption models[wiki.linuxfoundation.org](https://wiki.linuxfoundation.org/realtime/documentation/technical_basics/preemption_models#:~:text=,time%20behavior).
- STUN (RL-based tuning)[mdpi.com](https://www.mdpi.com/2076-3417/12/14/7072#:~:text=achieve%20optimal%20performance%20in%20the,training%20time%20and%20enhances%20the) and Learning EAS (policy-gradient RL)[researchgate.net](https://www.researchgate.net/publication/338526638_Performance_Improvement_of_Linux_CPU_Scheduler_Using_Policy_Gradient_Reinforcement_Learning_for_Android_Smartphones#:~:text=this%20paper%20introduces%20the%20Learning,through%20the%20policy%20gradient%20reinforcement).
- KernelOracle (deep learning for scheduling)[arxiv.org](https://arxiv.org/abs/2505.15213#:~:text=,next%20task%20to%20be%20scheduled).
- EEVDF scheduler in Linux 6.6 (Zijlstra)[docs.kernel.org](https://docs.kernel.org/scheduler/sched-eevdf.html#:~:text=The%20%E2%80%9CEarliest%20Eligible%20Virtual%20Deadline,be%20found%20in%20CFS%20Scheduler)[lwn.net](https://lwn.net/Articles/969062/#:~:text=A%20task%20is%20deemed%20,will%20tend%20to%20run%20first).
- CXL and heterogeneous memory: CXL introduction[computeexpresslink.org](https://computeexpresslink.org/wp-content/uploads/2024/03/CXL_An-Introduction-to-CXL-Technology-Webinar.pdf#:~:text=Heterogeneous%20Computing%20Revisited%20%E2%80%93%20with,%E2%80%A6%20%E2%80%A6%20%E2%80%A6) and performance study[arxiv.org](https://arxiv.org/html/2405.14209v1#:~:text=The%20CXL%20memory%20is%20a,highlights%20the%20importance%20of%20appropriately).
- Additional sources on fairness, multicore, and scheduling can be found in the cited LWN articles and Linux kernel documentation.