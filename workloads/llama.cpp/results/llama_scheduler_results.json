{
  "default": {
    "pp_tps": 4.76777,
    "tg_tps": 5.99503,
    "pp_time": 54982.520516608,
    "tg_time": 417.01209745,
    "raw_output": [
      {
        "build_commit": "5aa1105d",
        "build_number": 6082,
        "cpu_info": "Intel(R) Xeon(R) 6787P",
        "gpu_info": "",
        "backends": "CPU",
        "model_filename": "/root/yunwei37/ai-os/workloads/llama.cpp/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "model_type": "llama 1B Q4_K - Medium",
        "model_size": 667078656,
        "model_n_params": 1100048384,
        "n_batch": 4,
        "n_ubatch": 512,
        "n_threads": 172,
        "cpu_mask": "0x0",
        "cpu_strict": false,
        "poll": 50,
        "type_k": "f16",
        "type_v": "f16",
        "n_gpu_layers": 99,
        "split_mode": "layer",
        "main_gpu": 0,
        "no_kv_offload": false,
        "flash_attn": false,
        "tensor_split": "0.00",
        "tensor_buft_overrides": "none",
        "defrag_thold": -1.0,
        "use_mmap": true,
        "embeddings": false,
        "no_op_offload": 0,
        "n_prompt": 512,
        "n_gen": 0,
        "n_depth": 0,
        "test_time": "2025-08-20T08:02:26Z",
        "avg_ns": 107387735384,
        "stddev_ns": 0,
        "avg_ts": 4.76777,
        "stddev_ts": 0.0,
        "samples_ns": [
          107387735384
        ],
        "samples_ts": [
          4.76777
        ]
      },
      {
        "build_commit": "5aa1105d",
        "build_number": 6082,
        "cpu_info": "Intel(R) Xeon(R) 6787P",
        "gpu_info": "",
        "backends": "CPU",
        "model_filename": "/root/yunwei37/ai-os/workloads/llama.cpp/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "model_type": "llama 1B Q4_K - Medium",
        "model_size": 667078656,
        "model_n_params": 1100048384,
        "n_batch": 4,
        "n_ubatch": 512,
        "n_threads": 172,
        "cpu_mask": "0x0",
        "cpu_strict": false,
        "poll": 50,
        "type_k": "f16",
        "type_v": "f16",
        "n_gpu_layers": 99,
        "split_mode": "layer",
        "main_gpu": 0,
        "no_kv_offload": false,
        "flash_attn": false,
        "tensor_split": "0.00",
        "tensor_buft_overrides": "none",
        "defrag_thold": -1.0,
        "use_mmap": true,
        "embeddings": false,
        "no_op_offload": 0,
        "n_prompt": 0,
        "n_gen": 50,
        "n_depth": 0,
        "test_time": "2025-08-20T08:06:01Z",
        "avg_ns": 8340241949,
        "stddev_ns": 0,
        "avg_ts": 5.99503,
        "stddev_ts": 0.0,
        "samples_ns": [
          8340241949
        ],
        "samples_ts": [
          5.99503
        ]
      }
    ],
    "scheduler": "default",
    "exit_code": 0
  },
  "scx_bpfland": {
    "pp_tps": 1.995851,
    "tg_tps": 1.993836,
    "pp_time": 131344.505972736,
    "tg_time": 1253.86413565,
    "raw_output": [
      {
        "build_commit": "5aa1105d",
        "build_number": 6082,
        "cpu_info": "Intel(R) Xeon(R) 6787P",
        "gpu_info": "",
        "backends": "CPU",
        "model_filename": "/root/yunwei37/ai-os/workloads/llama.cpp/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "model_type": "llama 1B Q4_K - Medium",
        "model_size": 667078656,
        "model_n_params": 1100048384,
        "n_batch": 4,
        "n_ubatch": 512,
        "n_threads": 172,
        "cpu_mask": "0x0",
        "cpu_strict": false,
        "poll": 50,
        "type_k": "f16",
        "type_v": "f16",
        "n_gpu_layers": 99,
        "split_mode": "layer",
        "main_gpu": 0,
        "no_kv_offload": false,
        "flash_attn": false,
        "tensor_split": "0.00",
        "tensor_buft_overrides": "none",
        "defrag_thold": -1.0,
        "use_mmap": true,
        "embeddings": false,
        "no_op_offload": 0,
        "n_prompt": 512,
        "n_gen": 0,
        "n_depth": 0,
        "test_time": "2025-08-20T08:06:12Z",
        "avg_ns": 256532238228,
        "stddev_ns": 0,
        "avg_ts": 1.995851,
        "stddev_ts": 0.0,
        "samples_ns": [
          256532238228
        ],
        "samples_ts": [
          1.99585
        ]
      },
      {
        "build_commit": "5aa1105d",
        "build_number": 6082,
        "cpu_info": "Intel(R) Xeon(R) 6787P",
        "gpu_info": "",
        "backends": "CPU",
        "model_filename": "/root/yunwei37/ai-os/workloads/llama.cpp/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "model_type": "llama 1B Q4_K - Medium",
        "model_size": 667078656,
        "model_n_params": 1100048384,
        "n_batch": 4,
        "n_ubatch": 512,
        "n_threads": 172,
        "cpu_mask": "0x0",
        "cpu_strict": false,
        "poll": 50,
        "type_k": "f16",
        "type_v": "f16",
        "n_gpu_layers": 99,
        "split_mode": "layer",
        "main_gpu": 0,
        "no_kv_offload": false,
        "flash_attn": false,
        "tensor_split": "0.00",
        "tensor_buft_overrides": "none",
        "defrag_thold": -1.0,
        "use_mmap": true,
        "embeddings": false,
        "no_op_offload": 0,
        "n_prompt": 0,
        "n_gen": 50,
        "n_depth": 0,
        "test_time": "2025-08-20T08:14:45Z",
        "avg_ns": 25077282713,
        "stddev_ns": 0,
        "avg_ts": 1.993836,
        "stddev_ts": 0.0,
        "samples_ns": [
          25077282713
        ],
        "samples_ts": [
          1.99384
        ]
      }
    ],
    "scheduler": "scx_bpfland",
    "exit_code": 0
  }
}