# AI-Generated Scheduler Testing Framework

This framework demonstrates **AI-generated custom Linux kernel schedulers** that optimize workload-specific performance. It includes both the test workloads and the corresponding **custom BPF schedulers automatically generated by AI** to optimize each workload pattern.

The framework uses a 40-task parallel configuration with severe load imbalance (39 short tasks + 1 long task) to demonstrate how AI-generated schedulers can achieve **30-35% performance improvements** over the default CFS scheduler.

## Files Structure

```
workloads/processing/
├── README.md                           # This file
├── case.md                             # Detailed workload documentation
├── test_cases_parallel.json            # Test case definitions (40 tasks: 39 short + 1 long)
├── evaluate_workloads_parallel.py      # Workload execution framework
├── schedulers/                         # AI-generated custom BPF schedulers
│   ├── compression.bpf.c              # Scheduler optimized for compression workload
│   ├── video_transcode.bpf.c          # Scheduler optimized for video transcoding
│   ├── ctest_suite.bpf.c              # Scheduler optimized for test suites
│   ├── git_add_different.bpf.c        # Scheduler optimized for git operations
│   ├── file_checksum.bpf.c            # Scheduler optimized for file I/O
│   ├── hotkey_aggregation.bpf.c       # Scheduler optimized for skewed data
│   ├── ddos_log_analysis.bpf.c        # Scheduler optimized for log processing
│   ├── viral_product_analytics.bpf.c  # Scheduler optimized for analytics
│   ├── loader.c                       # BPF scheduler loader
│   ├── Makefile                       # Build system
│   └── README.md                      # Scheduler development guide
├── scripts/                            # Testing and analysis scripts
│   ├── run_scheduler_tests.py         # Automated scheduler comparison
│   ├── analyze_scheduler_results.py   # Performance analysis
│   └── scheduler_test_results/        # Test results with custom schedulers
└── assets/                             # Workload implementations
    ├── short.c, long.c                # C test programs
    ├── compression.py                 # File compression workload
    ├── file_checksum.py               # File I/O workload
    ├── video_transcode.cpp            # Video processing workload
    ├── spark_skew_*.py                # Hot key aggregation workload
    ├── pandas_etl_*.py                # Log analysis workload
    └── flink_join_*.py                # Analytics join workload
```

## Quick Start

### 1. Install Dependencies

```bash
# Install required packages
cd scripts && sudo ./install_deps.sh && cd ..
```

### 2. Build AI-Generated Schedulers

```bash
cd schedulers && make && cd ..
```

### 3. Run Workload Tests

**Option A: Run workloads only (no scheduler comparison)**
```bash
# List available test cases
python3 evaluate_workloads_parallel.py --list

# Run a specific test case
python3 evaluate_workloads_parallel.py --test hotkey_aggregation

# Run all test cases
python3 evaluate_workloads_parallel.py --all
```

**Option B: Compare default vs AI-generated schedulers**
```bash
cd scripts

# Run single workload with both default and custom scheduler
python3 run_scheduler_tests.py --test hotkey_aggregation

# Run all workloads with scheduler comparison
python3 run_scheduler_tests.py --all

# Results saved to scheduler_test_results/ directory
```

### 4. Analyze Results

```bash
cd scripts
python3 analyze_scheduler_results.py
# Generates performance comparison charts showing ~30-35% improvements
```

## AI-Generated Schedulers

Each workload has a **custom BPF scheduler generated by AI** (in `schedulers/` directory) that:
- Detects long-tail tasks by process name or filename
- Prioritizes long-running tasks to reduce overall completion time
- Implements workload-specific optimizations (e.g., CPU affinity, priority boosting)
- Achieves **30-35% performance improvement** over default CFS scheduler

### Example: Compression Scheduler

The `compression.bpf.c` scheduler detects compression tasks and prioritizes the large file compression to prevent it from blocking overall completion. See `schedulers/README.md` for details on how these schedulers work.

## Test Cases

The framework includes **9 test cases** across different categories, each configured with 40 parallel tasks (39 short + 1 long):

### File Processing
- **compression**: Compression of mixed-size files with severe load imbalance (Python implementation)
- **file_checksum**: Parallel file system operations with one large file blocking completion

### Media Processing  
- **video_transcode**: Video transcoding with one large file dominating processing time (C++ implementation)

### Software Testing
- **ctest_suite**: Test suite with fast unit tests and one slow integration test

### Version Control
- **git_add_different**: Git add operations with different numbers of files

### Data Processing
- **log_processing**: Log processing with skewed chunks and different compression levels
- **hotkey_aggregation**: Analytics with skewed data distribution (hot key problem)
- **ddos_log_analysis**: Security log analysis with temporal spike pattern (DDoS simulation)
- **viral_product_analytics**: Retail analytics with temporal hot product pattern (trending item simulation)

## Expected Results

All test cases demonstrate the "long-tail problem" where:
- 39 short tasks complete quickly (few seconds each)
- 1 long task takes much longer (significantly more time)
- Expected scheduler optimization: **30-35% improvement**
- Total workload: 40 parallel tasks with severe imbalance

## Framework Features

### Process Monitoring
- Automatically detects long-running processes (>0.5s)
- Measures CPU usage and runtime distribution
- Calculates skew ratios to identify optimization potential

### Dependency Management
- Automatic dependency checking before test execution
- Graceful fallback when optional packages unavailable
- Clear error messages for missing dependencies

### Result Analysis
- JSON output with detailed timing and process data
- Summary statistics across all tests
- Long-tail detection and analysis

### Test Configuration
All test cases are defined in `test_cases_parallel.json` with:
- Separate setup commands for small and large tasks
- Small commands (executed 39 times in parallel)  
- Large commands (executed 1 time in parallel)
- Cleanup commands (resource cleanup)
- Expected performance characteristics and improvement ratios
- Dependencies and metadata

## Usage Examples

### Running Specific Categories

```bash
# Run only data processing tests
python3 evaluate_workloads_parallel.py --test hotkey_aggregation
python3 evaluate_workloads_parallel.py --test ddos_log_analysis
python3 evaluate_workloads_parallel.py --test viral_product_analytics

# Run file processing tests
python3 evaluate_workloads_parallel.py --test pigz_compression
python3 evaluate_workloads_parallel.py --test file_checksum
```

### Custom Test Execution

```bash
# Run with custom timeout and save results
python3 evaluate_workloads_parallel.py --test pigz_compression --save pigz_results.json
```

### Analyzing Results

The framework saves results in JSON format with:
- Test execution times
- Process monitoring data
- Long-tail detection results
- Performance improvement estimates

Example result structure:
```json
{
  "test_id": "hotkey_aggregation",
  "status": "success",
  "test_time": 10.58,
  "expected_improvement": 0.33,
  "process_analysis": {
    "long_tail_detected": true,
    "skew_ratio": 100.0
  }
}
```

## Troubleshooting

### Missing Dependencies
Run the dependency installer:
```bash
sudo ./install_deps.sh
```

### Process Monitoring Issues
If psutil is unavailable, use `--no-monitor`:
```bash
python3 evaluate_workloads_parallel.py --test TEST_ID --no-monitor
```

### Permission Issues
Some tests require sudo for system commands. Run with appropriate permissions.

### Test Failures
Check the error output in the JSON results for specific failure reasons.

## Customization

### Adding New Test Cases
1. Edit `test_cases_parallel.json`
2. Add small_setup, large_setup, small_commands, and large_commands
3. Specify dependencies and expected improvement ratios
4. Create any required asset scripts in `assets/`

### Modifying Data Sizes
Adjust the commands in `test_cases_parallel.json` to change:
- File sizes (dd commands, count parameters)
- Record counts (seq commands, loop ranges)
- Processing intensity (parameters in Python preparation scripts)

## Performance Notes

- Tests are designed for 4-CPU systems
- Configuration: 40 parallel tasks (39 short + 1 long)
- Data sizes optimized for demonstrable imbalance
- Expected improvement: 30-35% with custom schedulers
- Total framework runtime: varies by test case complexity
- Each test case specifically designed to create scheduler optimization opportunities