{
  "configuration": {
    "short_tasks": 39,
    "long_tasks": 1,
    "total_tasks": 40
  },
  "test_cases": [
    {
      "id": "pigz_compression",
      "name": "Pigz Directory Compression",
      "description": "Parallel compression of mixed-size files with severe load imbalance",
      "category": "file_processing",
      "setup_commands": [
        "mkdir -p test_data",
        "seq 1 150000 > test_data/short_file.dat",
        "seq 1 4000000 > test_data/large_file.dat"
      ],
      "small_commands": [
        "pigz -1 test_data/short_file.dat"
      ],
      "large_commands": [
        "pigz -1 test_data/large_file.dat"
      ],
      "cleanup_commands": [
        "rm -rf test_data/",
        "rm -f *.gz"
      ],
      "expected_improvement": 0.33,
      "dependencies": ["pigz"]
    },
    {
      "id": "ffmpeg_transcode",
      "name": "FFmpeg Split Transcode",
      "description": "Video transcoding with one large file dominating processing time",
      "category": "media_processing",
      "setup_commands": [
        "mkdir -p clips out",
        "ffmpeg -f lavfi -i testsrc=duration=1:size=320x240:rate=30 -loglevel quiet clips/short.mp4",
        "ffmpeg -f lavfi -i testsrc=duration=5:size=1920x1080:rate=30 -loglevel quiet clips/long.mp4"
      ],
      "small_commands": [
        "ffmpeg -loglevel quiet -i clips/short.mp4 -vf scale=640:-1 -c:v libx264 -preset veryfast out/short_out.mp4"
      ],
      "large_commands": [
        "ffmpeg -loglevel quiet -i clips/long.mp4 -vf scale=640:-1 -c:v libx264 -preset veryfast out/long_out.mp4"
      ],
      "cleanup_commands": [
        "rm -rf clips/ out/"
      ],
      "expected_improvement": 0.33,
      "dependencies": ["ffmpeg"]
    },
    {
      "id": "c_compile_suite",
      "name": "C Compilation Suite with Complex Project",
      "description": "Compile suite with fast simple programs and one complex project",
      "category": "software_compilation",
      "setup_commands": [
        "gcc -O2 assets/short.c -lm -o assets/short",
        "gcc -O2 assets/long.c -lm -o assets/long"
      ],
      "small_commands": [
        "./assets/short"
      ],
      "large_commands": [
        "./assets/long"
      ],
      "cleanup_commands": [
        "rm -f assets/short assets/long"
      ],
      "expected_improvement": 0.33,
      "dependencies": ["gcc"]
    },
    {
      "id": "git_compression",
      "name": "Git Incremental Compression",
      "description": "Git garbage collection with mixed object sizes",
      "category": "version_control",
      "setup_commands": [
        "rm -rf test_repo && mkdir -p test_repo && cd test_repo && git init",
        "cd test_repo && git config user.name 'Test User' && git config user.email 'test@example.com'",
        "cd test_repo && echo 'small change' > small_file.txt && git add small_file.txt && git commit -m 'small commit' --quiet",
        "cd test_repo && dd if=/dev/urandom of=large.bin bs=1M count=300 2>/dev/null && git add large.bin && git commit -m 'add large binary' --quiet"
      ],
      "small_commands": [
        "cd test_repo && git log --oneline -n 5 > /tmp/short_log.txt && sha256sum /tmp/short_log.txt"
      ],
      "large_commands": [
        "cd test_repo && echo \"Processing large repository\" && git repack -a -d -f --depth=50 --window=250"
      ],
      "cleanup_commands": [
        "rm -rf test_repo/"
      ],
      "expected_improvement": 0.30,
      "dependencies": ["git"]
    },
    {
      "id": "file_checksum",
      "name": "Parallel File System Operations",
      "description": "Checksum operations with one large file blocking completion",
      "category": "file_processing",
      "setup_commands": [
        "mkdir -p large-dir",
        "dd if=/dev/urandom of=large-dir/short_file.dat bs=1M count=5 2>/dev/null",
        "dd if=/dev/urandom of=large-dir/long_file.dat bs=1M count=100 2>/dev/null"
      ],
      "small_commands": [
        "sha256sum large-dir/short_file.dat"
      ],
      "large_commands": [
        "sha256sum large-dir/long_file.dat"
      ],
      "cleanup_commands": [
        "rm -rf large-dir/",
        "rm -f checksums.txt"
      ],
      "expected_improvement": 0.33,
      "dependencies": ["sha256sum"]
    },
    {
      "id": "spark_shuffle",
      "name": "Spark Local Shuffle with Skew",
      "description": "Analytics with skewed data distribution (hot key problem)",
      "category": "data_processing",
      "setup_commands": [
        "chmod +x assets/spark_skew_test.py",
        "mkdir -p spark_data",
        "seq 1 100000 > spark_data/short_data.txt",
        "seq 1 500000 > spark_data/long_data.txt"
      ],
      "small_commands": [
        "sha256sum spark_data/short_data.txt | cut -d' ' -f1 > /tmp/short_hash.txt"
      ],
      "large_commands": [
        "sha256sum spark_data/long_data.txt | cut -d' ' -f1 > /tmp/long_result.txt"
      ],
      "cleanup_commands": [
        "rm -rf spark_data/"
      ],
      "expected_improvement": 0.33,
      "dependencies": ["python3"]
    },
    {
      "id": "log_analysis",
      "name": "Log Analysis with Heavy Processing",
      "description": "Log processing with grep/awk analysis and one complex computation",
      "category": "data_processing",
      "setup_commands": [
        "mkdir -p logs",
        "seq 1 10000 | awk '{print strftime(\"%Y-%m-%d %H:%M:%S\"), \"INFO\", \"Message\", $1}' | gzip > logs/short_log.gz",
        "seq 1 500000 | awk '{print strftime(\"%Y-%m-%d %H:%M:%S\"), \"INFO\", \"Message\", $1}' | gzip > logs/long_log.gz"
      ],
      "small_commands": [
        "zcat logs/short_log.gz | grep -c 'INFO'"
      ],
      "large_commands": [
        "zcat logs/long_log.gz | awk '{count[$2]++} END {for(i in count) print i, count[i]}' | sort -nr | sha256sum > /tmp/analysis_result"
      ],
      "cleanup_commands": [
        "rm -rf logs/",
        "rm -f /tmp/analysis_result"
      ],
      "expected_improvement": 0.35,
      "dependencies": ["gzip", "awk", "grep"]
    },
    {
      "id": "text_processing",
      "name": "Text Processing with Complex Analysis",
      "description": "Text analysis with simple grep and one complex sed/awk pipeline",
      "category": "data_processing",
      "setup_commands": [
        "mkdir -p text_data",
        "seq 1 10000 | awk '{printf \"key_short,value_%d\\n\", $1}' > text_data/short_file.txt",
        "seq 1 250000 | awk '{printf \"key_long,value_%d\\n\", $1}' > text_data/long_file.txt"
      ],
      "small_commands": [
        "grep -c 'key' text_data/short_file.txt"
      ],
      "large_commands": [
        "cat text_data/long_file.txt | awk -F',' '{count[$1]++; sum[$1]+=$2} END {for(k in count) printf \"%s:%d:%s\\n\", k, count[k], sum[k]}' | sed 's/:/|/g' | sha256sum > /tmp/complex_result"
      ],
      "cleanup_commands": [
        "rm -rf text_data/",
        "rm -f /tmp/complex_result"
      ],
      "expected_improvement": 0.30,
      "dependencies": ["grep", "awk", "sed"]
    },
    {
      "id": "ctest_suite",
      "name": "CTest Suite with Slow Integration Test",
      "description": "Test suite with fast unit tests and one slow integration test",
      "category": "software_testing",
      "setup_commands": [
        "mkdir -p ctest_project",
        "echo '#include <stdio.h>\n#include <math.h>\nint main() { double sum = 0; for(int i = 0; i < 1000000; i++) { sum += sin(i * 0.001) * cos(i * 0.001); } printf(\"Unit test result: %.6f\\n\", sum); return 0; }' > ctest_project/unit_test.c",
        "echo '#include <stdio.h>\n#include <math.h>\n#include <stdlib.h>\nint main() { double sum = 0; for(long i = 0; i < 100000000L; i++) { sum += sin(i * 0.00001) * cos(i * 0.00001) * sqrt(i % 1000 + 1); } printf(\"Integration test result: %.6f\\n\", sum); return 0; }' > ctest_project/integration_test.c"
      ],
      "small_commands": [
        "gcc -O2 -lm ctest_project/unit_test.c -o ctest_project/unit_test && ctest_project/unit_test"
      ],
      "large_commands": [
        "gcc -O2 -lm ctest_project/integration_test.c -o ctest_project/integration_test && ctest_project/integration_test"
      ],
      "cleanup_commands": [
        "rm -rf ctest_project/"
      ],
      "expected_improvement": 0.33,
      "dependencies": ["gcc"]
    },
    {
      "id": "log_processing",
      "name": "Log Processing with Skewed Chunks",
      "description": "Processing log files with different sizes and compression",
      "category": "data_processing",
      "setup_commands": [
        "mkdir -p log_chunks",
        "seq 1 50000 | awk '{print strftime(\"%Y-%m-%d %H:%M:%S\"), \"[INFO]\", \"Request from\", \"192.168.1.\"int(rand()*255), \"processed in\", int(rand()*100), \"ms\"}' > log_chunks/small.log",
        "seq 1 2000000 | awk '{print strftime(\"%Y-%m-%d %H:%M:%S\"), \"[INFO]\", \"Request from\", \"192.168.1.\"int(rand()*255), \"processed in\", int(rand()*100), \"ms\"}' > log_chunks/large.log"
      ],
      "small_commands": [
        "gzip -c log_chunks/small.log | zcat | grep -E '\\[INFO\\]' | awk '{print $4}' | sort | uniq -c | sort -nr > log_chunks/small_ips.txt"
      ],
      "large_commands": [
        "gzip -c log_chunks/large.log | zcat | grep -E '\\[INFO\\]' | awk '{print $4}' | sort | uniq -c | sort -nr > log_chunks/large_ips.txt"
      ],
      "cleanup_commands": [
        "rm -rf log_chunks/"
      ],
      "expected_improvement": 0.35,
      "dependencies": ["gzip", "grep", "awk", "sort", "uniq"]
    },
    {
      "id": "dask_groupby",
      "name": "Dask-like GroupBy with Power-law Distribution",
      "description": "Customer analytics simulation with skewed group sizes",
      "category": "data_processing",
      "setup_commands": [
        "mkdir -p dask_data",
        "chmod +x assets/dask_groupby_test.py",
        "for i in {1..99}; do echo \"customer_$i,$(shuf -i 100-500 -n 1)\" >> dask_data/small_groups.csv; done",
        "for i in {1..50000}; do echo \"customer_999,$(shuf -i 100-500 -n 1)\" >> dask_data/large_group.csv; done"
      ],
      "small_commands": [
        "python3 assets/dask_groupby_test.py"
      ],
      "large_commands": [
        "python3 assets/dask_groupby_test.py"
      ],
      "cleanup_commands": [
        "rm -rf dask_data/",
        "rm -f /tmp/dask_*"
      ],
      "expected_improvement": 0.33,
      "dependencies": ["python3", "awk"]
    },
    {
      "id": "pandas_etl",
      "name": "Pandas ETL with DDoS Spike Simulation",
      "description": "ETL pipeline with sudden spike in data volume",
      "category": "data_processing",
      "setup_commands": [
        "mkdir -p logs",
        "chmod +x assets/pandas_etl_test.py",
        "for i in {1..39}; do seq 1 1000 | awk -v i=$i '{print strftime(\"%Y-%m-%d %H:%M:%S\"), \"[INFO]\", \"server\"i, \"Request processed\", $1}' | gzip > logs/log_$i.gz; done",
        "seq 1 100000 | awk '{print strftime(\"%Y-%m-%d %H:%M:%S\"), \"[ERROR]\", \"server40\", \"DDoS attack detected\", $1}' | gzip > logs/log_40.gz"
      ],
      "small_commands": [
        "python3 assets/pandas_etl_test.py"
      ],
      "large_commands": [
        "python3 assets/pandas_etl_test.py"
      ],
      "cleanup_commands": [
        "rm -rf logs/",
        "rm -f /tmp/pandas_*"
      ],
      "expected_improvement": 0.33,
      "dependencies": ["python3", "gzip", "awk"]
    },
    {
      "id": "flink_join",
      "name": "Flink-like Join with Popular Items",
      "description": "Retail analytics simulation with hot products",
      "category": "data_processing",
      "setup_commands": [
        "mkdir -p flink_data",
        "chmod +x assets/flink_join_test.py",
        "for i in {1..99}; do for j in {1..100}; do echo \"product_$i,$(shuf -i 10-50 -n 1),$(date +%s -d \"$j hours ago\")\" >> flink_data/regular_products.csv; done; done",
        "for i in {1..10000}; do echo \"product_999,$(shuf -i 10-50 -n 1),$(date +%s -d \"$((RANDOM % 24)) hours ago\")\" >> flink_data/popular_product.csv; done"
      ],
      "small_commands": [
        "python3 assets/flink_join_test.py"
      ],
      "large_commands": [
        "python3 assets/flink_join_test.py"
      ],
      "cleanup_commands": [
        "rm -rf flink_data/",
        "rm -f /tmp/flink_*"
      ],
      "expected_improvement": 0.33,
      "dependencies": ["python3", "awk"]
    }
  ],
  "metadata": {
    "version": "2.1",
    "description": "Long-tail workload test cases with system-level parallel commands",
    "target_platform": "Linux",
    "cpu_count": 4,
    "total_test_cases": 14,
    "average_expected_improvement": 0.32
  }
}