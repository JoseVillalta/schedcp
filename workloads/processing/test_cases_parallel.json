{
  "configuration": {
    "short_tasks": 39,
    "long_tasks": 1,
    "total_tasks": 40
  },
  "test_cases": [
    {
      "id": "pigz_compression",
      "name": "Pigz Directory Compression",
      "description": "Parallel compression of mixed-size files with severe load imbalance",
      "category": "file_processing",
      "small_setup": [
        "mkdir -p test_data",
        "seq 1 30000000 > test_data/short_file.dat"
      ],
      "large_setup": [
        "mkdir -p test_data",
        "seq 1 200000000 > test_data/large_file.dat"
      ],
      "small_commands": [
        "pigz -9 test_data/short_file.dat"
      ],
      "large_commands": [
        "pigz -9 test_data/large_file.dat"
      ],
      "cleanup_commands": [
        "rm -rf test_data/",
        "rm -f *.gz"
      ],
      "dependencies": ["pigz"]
    },
    {
      "id": "ffmpeg_transcode",
      "name": "FFmpeg Split Transcode",
      "description": "Video transcoding with one large file dominating processing time",
      "category": "media_processing",
      "small_setup": [
        "mkdir -p clips out",
        "ffmpeg -f lavfi -i testsrc=duration=6:size=320x240:rate=30 -loglevel quiet clips/short.mp4"
      ],
      "large_setup": [
        "mkdir -p clips out",
        "ffmpeg -f lavfi -i testsrc=duration=70:size=1920x1080:rate=30 -loglevel quiet clips/long.mp4"
      ],
      "small_commands": [
        "ffmpeg -loglevel quiet -i clips/short.mp4 -vf scale=640:-1 -c:v libx264 -preset veryfast out/short_out.mp4"
      ],
      "large_commands": [
        "ffmpeg -loglevel quiet -i clips/long.mp4 -vf scale=640:-1 -c:v libx264 -preset veryfast out/long_out.mp4"
      ],
      "cleanup_commands": [
        "rm -rf clips/ out/"
      ],
      "dependencies": ["ffmpeg"]
    },
    {
      "id": "ctest_suite",
      "name": "CTest Suite with Slow Integration Test",
      "description": "Test suite with fast unit tests and one slow integration test",
      "category": "software_testing",
      "small_setup": [
        "cp $ORIGINAL_CWD/assets/short.c .",
        "gcc -O2 short.c -lm -o short"
      ],
      "large_setup": [
        "cp $ORIGINAL_CWD/assets/long.c .",
        "gcc -O2 long.c -lm -o long"
      ],
      "small_commands": [
        "./short"
      ],
      "large_commands": [
        "./long"
      ],
      "cleanup_commands": [
        "rm -f assets/short assets/long"
      ],
      "dependencies": ["gcc"]
    },
    {
      "id": "git_compression",
      "name": "Git Incremental Compression",
      "description": "Git garbage collection with mixed object sizes",
      "category": "version_control",
      "small_setup": [
        "mkdir -p test_repo && cd test_repo && git init",
        "cd test_repo && git config user.name 'Test User' && git config user.email 'test@example.com'",
        "cd test_repo && for i in {1..50}; do echo \"change $i\" > file_$i.txt && git add file_$i.txt && git commit -m \"commit $i\" --quiet; done"
      ],
      "large_setup": [
        "mkdir -p test_repo && cd test_repo && git init",
        "cd test_repo && git config user.name 'Test User' && git config user.email 'test@example.com'",
        "cd test_repo && for i in {1..200}; do dd if=/dev/urandom of=bin_$i.dat bs=1M count=2 2>/dev/null && git add bin_$i.dat && git commit -m \"binary $i\" --quiet; done"
      ],
      "small_commands": [
        "cd test_repo && git log --oneline -n 5 > /tmp/short_log.txt && sha256sum /tmp/short_log.txt"
      ],
      "large_commands": [
        "cd test_repo && echo \"Processing large repository\" && git repack -a -d -f --depth=10 --window=50"
      ],
      "cleanup_commands": [
        "rm -rf test_repo/"
      ],
      "dependencies": ["git"]
    },
    {
      "id": "file_checksum",
      "name": "Parallel File System Operations",
      "description": "Checksum operations with one large file blocking completion",
      "category": "file_processing",
      "small_setup": [
        "mkdir -p large-dir",
        "dd if=/dev/urandom of=large-dir/short_file.dat bs=1M count=200 2>/dev/null"
      ],
      "large_setup": [
        "mkdir -p large-dir",
        "dd if=/dev/urandom of=large-dir/long_file.dat bs=1M count=1000 2>/dev/null"
      ],
      "small_commands": [
        "sha256sum large-dir/short_file.dat"
      ],
      "large_commands": [
        "sha256sum large-dir/long_file.dat"
      ],
      "cleanup_commands": [
        "rm -rf large-dir/",
        "rm -f checksums.txt"
      ],
      "dependencies": ["sha256sum"]
    },
    {
      "id": "hotkey_aggregation",
      "name": "Hot Key Aggregation",
      "description": "Analytics with skewed data distribution (hot key problem)",
      "category": "data_processing",
      "small_setup": [
        "cp $ORIGINAL_CWD/assets/spark_skew_prepare.py $ORIGINAL_CWD/assets/spark_skew_test.py .",
        "cp spark_skew_test.py small_spark_skew_test.py",
        "chmod +x spark_skew_prepare.py small_spark_skew_test.py",
        "python3 spark_skew_prepare.py --regular-keys 8000 --hot-keys 15000 --output spark_small.csv"
      ],
      "large_setup": [
        "cp $ORIGINAL_CWD/assets/spark_skew_prepare.py $ORIGINAL_CWD/assets/spark_skew_test.py .",
        "cp spark_skew_test.py large_spark_skew_test.py",
        "chmod +x spark_skew_prepare.py large_spark_skew_test.py",
        "python3 spark_skew_prepare.py --regular-keys 100000 --hot-keys 800000 --output spark_large.csv"
      ],
      "small_commands": [
        "python3 small_spark_skew_test.py spark_small.csv"
      ],
      "large_commands": [
        "python3 large_spark_skew_test.py spark_large.csv"
      ],
      "cleanup_commands": [
        "rm -f spark_small.csv spark_large.csv",
        "rm -f small_spark_skew_test.py large_spark_skew_test.py spark_skew_prepare.py spark_skew_test.py"
      ],
      "dependencies": ["python3"]
    },
    {
      "id": "log_processing",
      "name": "Log Processing with Skewed Chunks",
      "description": "Processing log files with different sizes and compression",
      "category": "data_processing",
      "small_setup": [
        "mkdir -p log_chunks",
        "seq 1 500000 | awk '{print strftime(\"%Y-%m-%d %H:%M:%S\"), \"[INFO]\", \"Request from\", \"192.168.1.\"int(rand()*255), \"processed in\", int(rand()*100), \"ms\"}' > log_chunks/small.log"
      ],
      "large_setup": [
        "mkdir -p log_chunks",
        "seq 1 7000000 | awk '{print strftime(\"%Y-%m-%d %H:%M:%S\"), \"[INFO]\", \"Request from\", \"192.168.1.\"int(rand()*255), \"processed in\", int(rand()*100), \"ms\"}' > log_chunks/large.log"
      ],
      "small_commands": [
        "gzip -c log_chunks/small.log | zcat | grep -E '\\[INFO\\]' | awk '{print $4}' | sort | uniq -c | sort -nr > log_chunks/small_ips.txt"
      ],
      "large_commands": [
        "gzip -c log_chunks/large.log | zcat | grep -E '\\[INFO\\]' | awk '{print $4}' | sort | uniq -c | sort -nr > log_chunks/large_ips.txt"
      ],
      "cleanup_commands": [
        "rm -rf log_chunks/"
      ],
      "dependencies": ["gzip", "grep", "awk", "sort", "uniq"]
    },
    {
      "id": "ddos_log_analysis",
      "name": "DDoS Log Analysis",
      "description": "Security log analysis with temporal spike pattern (DDoS simulation)",
      "category": "data_processing",
      "small_setup": [
        "cp $ORIGINAL_CWD/assets/pandas_etl_prepare.py $ORIGINAL_CWD/assets/pandas_etl_test.py .",
        "cp pandas_etl_test.py small_pandas_etl_test.py",
        "chmod +x pandas_etl_prepare.py small_pandas_etl_test.py",
        "python3 pandas_etl_prepare.py --normal-logs 30000 --error-logs 8000 --output etl_small.gz"
      ],
      "large_setup": [
        "cp $ORIGINAL_CWD/assets/pandas_etl_prepare.py $ORIGINAL_CWD/assets/pandas_etl_test.py .",
        "cp pandas_etl_test.py large_pandas_etl_test.py",
        "chmod +x pandas_etl_prepare.py large_pandas_etl_test.py",
        "python3 pandas_etl_prepare.py --normal-logs 1000000 --error-logs 500000 --output etl_large.gz"
      ],
      "small_commands": [
        "python3 small_pandas_etl_test.py etl_small.gz"
      ],
      "large_commands": [
        "python3 large_pandas_etl_test.py etl_large.gz"
      ],
      "cleanup_commands": [
        "rm -f etl_small.gz etl_large.gz",
        "rm -f /tmp/pandas_*",
        "rm -f small_pandas_etl_test.py large_pandas_etl_test.py pandas_etl_prepare.py pandas_etl_test.py"
      ],
      "dependencies": ["python3", "gzip", "awk"]
    },
    {
      "id": "viral_product_analytics",
      "name": "Viral Product Analytics",
      "description": "Retail analytics with temporal hot product pattern (trending item simulation)",
      "category": "data_processing",
      "small_setup": [
        "cp $ORIGINAL_CWD/assets/flink_join_prepare.py $ORIGINAL_CWD/assets/flink_join_test.py .",
        "cp flink_join_test.py small_flink_join_test.py",
        "chmod +x flink_join_prepare.py small_flink_join_test.py",
        "python3 flink_join_prepare.py --regular-transactions 8000 --hot-transactions 12000 --output flink_small.csv"
      ],
      "large_setup": [
        "cp $ORIGINAL_CWD/assets/flink_join_prepare.py $ORIGINAL_CWD/assets/flink_join_test.py .",
        "cp flink_join_test.py large_flink_join_test.py",
        "chmod +x flink_join_prepare.py large_flink_join_test.py",
        "python3 flink_join_prepare.py --regular-transactions 100000 --hot-transactions 700000 --output flink_large.csv"
      ],
      "small_commands": [
        "python3 small_flink_join_test.py flink_small.csv"
      ],
      "large_commands": [
        "python3 large_flink_join_test.py flink_large.csv"
      ],
      "cleanup_commands": [
        "rm -f flink_small.csv flink_large.csv",
        "rm -f /tmp/flink_*",
        "rm -f small_flink_join_test.py large_flink_join_test.py flink_join_prepare.py flink_join_test.py"
      ],
      "dependencies": ["python3", "awk"]
    }
  ],
  "metadata": {
    "version": "2.1",
    "description": "Long-tail workload test cases with system-level parallel commands",
    "target_platform": "Linux",
    "cpu_count": 8,
    "total_test_cases": 9
  }
}