{
  "test_cases": [
    {
      "id": "pigz_compression",
      "name": "Pigz Directory Compression",
      "description": "Parallel compression of mixed-size files with severe load imbalance",
      "category": "file_processing",
      "setup_commands": [
        "mkdir -p test_data",
        "for i in {1..99}; do dd if=/dev/urandom of=test_data/file$i.dat bs=1K count=10 2>/dev/null; done",
        "dd if=/dev/urandom of=test_data/large.iso bs=1M count=500 2>/dev/null"
      ],
      "small_commands": [
        "pigz -1 test_data/file{ID}.dat"
      ],
      "large_commands": [
        "pigz -1 test_data/large.iso"
      ],
      "test_command": "find ./test_data -name '*.dat' -o -name '*.iso' -print0 | xargs -0 -n1 -P2 pigz -1",
      "cleanup_commands": [
        "rm -rf test_data/",
        "rm -f *.gz"
      ],
      "expected_improvement": 0.33,
      "workload_characteristics": {
        "small_tasks": 99,
        "large_tasks": 1,
        "size_ratio": 100,
        "expected_runtime_seconds": 15
      },
      "dependencies": ["pigz"]
    },
    {
      "id": "ffmpeg_transcode",
      "name": "FFmpeg Split Transcode",
      "description": "Video transcoding with one large file dominating processing time",
      "category": "media_processing",
      "setup_commands": [
        "mkdir -p clips out",
        "for i in {1..99}; do ffmpeg -f lavfi -i testsrc=duration=0.1:size=320x240:rate=30 -loglevel quiet clips/clip$i.mp4; done",
        "ffmpeg -f lavfi -i testsrc=duration=60:size=1920x1080:rate=30 -loglevel quiet clips/long_clip.mp4"
      ],
      "small_commands": [
        "ffmpeg -loglevel quiet -i clips/clip{ID}.mp4 -vf scale=640:-1 -c:v libx264 -preset veryfast out/clip{ID}.mp4"
      ],
      "large_commands": [
        "ffmpeg -loglevel quiet -i clips/long_clip.mp4 -vf scale=640:-1 -c:v libx264 -preset veryfast out/long_clip.mp4"
      ],
      "test_command": "for f in clips/*.mp4; do ffmpeg -loglevel quiet -i \"$f\" -vf scale=640:-1 -c:v libx264 -preset veryfast out/\"${f##*/}\" & done; wait",
      "cleanup_commands": [
        "rm -rf clips/ out/"
      ],
      "expected_improvement": 0.33,
      "workload_characteristics": {
        "small_tasks": 99,
        "large_tasks": 1,
        "size_ratio": 100,
        "expected_runtime_seconds": 15
      },
      "dependencies": ["ffmpeg"]
    },
    {
      "id": "pytest_suite",
      "name": "Pytest Test Suite with Slow Integration Test",
      "description": "Test suite with fast unit tests and one slow integration test",
      "category": "software_testing",
      "setup_commands": [
        "mkdir -p test_suite",
        "cat > test_suite/conftest.py << 'EOF'\nimport pytest\n\ndef pytest_addoption(parser):\n    parser.addoption('--test-id', default='0')\nEOF",
        "for i in {1..99}; do echo \"import time\\ndef test_fast_$i():\\n    time.sleep(0.01)\\n    assert True\" > test_suite/test_fast_$i.py; done",
        "echo \"import time\\ndef test_slow_integration():\\n    time.sleep(10)\\n    assert True\" > test_suite/test_slow.py"
      ],
      "small_commands": [
        "cd test_suite && python -m pytest -q test_fast_{ID}.py"
      ],
      "large_commands": [
        "cd test_suite && python -m pytest -q test_slow.py"
      ],
      "test_command": "cd test_suite && python -m pytest -q -n2 --durations=0",
      "cleanup_commands": [
        "rm -rf test_suite/",
        "rm -rf .pytest_cache/"
      ],
      "expected_improvement": 0.33,
      "workload_characteristics": {
        "small_tasks": 99,
        "large_tasks": 1,
        "size_ratio": 100,
        "expected_runtime_seconds": 12
      },
      "dependencies": ["python3-pytest", "python3-pytest-xdist"]
    },
    {
      "id": "git_compression",
      "name": "Git Incremental Compression",
      "description": "Git garbage collection with mixed object sizes",
      "category": "version_control",
      "setup_commands": [
        "rm -rf test_repo && mkdir -p test_repo && cd test_repo && git init",
        "cd test_repo && git config user.name 'Test User' && git config user.email 'test@example.com'",
        "cd test_repo && for i in {1..99}; do echo \"small change $i\" > file$i.txt && git add file$i.txt && git commit -m \"commit $i\" --quiet; done",
        "cd test_repo && dd if=/dev/urandom of=large.bin bs=1M count=300 2>/dev/null && git add large.bin && git commit -m 'add large binary' --quiet"
      ],
      "small_commands": [
        "cd test_repo && echo \"Processing small file {ID}\" && sleep 0.01"
      ],
      "large_commands": [
        "cd test_repo && echo \"Processing large repository\" && git repack -a -d -f --depth=50 --window=250"
      ],
      "test_command": "cd test_repo && time git gc",
      "cleanup_commands": [
        "rm -rf test_repo/"
      ],
      "expected_improvement": 0.30,
      "workload_characteristics": {
        "small_tasks": 99,
        "large_tasks": 1,
        "size_ratio": 100,
        "expected_runtime_seconds": 20
      },
      "dependencies": ["git"]
    },
    {
      "id": "file_checksum",
      "name": "Parallel File System Operations",
      "description": "Checksum operations with one large file blocking completion",
      "category": "file_processing",
      "setup_commands": [
        "mkdir -p large-dir",
        "for i in {1..99}; do dd if=/dev/urandom of=large-dir/file$i.dat bs=1M count=1 2>/dev/null; done",
        "dd if=/dev/urandom of=large-dir/largefile.dat bs=1M count=1024 2>/dev/null"
      ],
      "small_commands": [
        "sha256sum large-dir/file{ID}.dat"
      ],
      "large_commands": [
        "sha256sum large-dir/largefile.dat"
      ],
      "test_command": "find ./large-dir -type f -print0 | xargs -0 -n1 -P2 sha256sum > checksums.txt",
      "cleanup_commands": [
        "rm -rf large-dir/",
        "rm -f checksums.txt"
      ],
      "expected_improvement": 0.33,
      "workload_characteristics": {
        "small_tasks": 99,
        "large_tasks": 1,
        "size_ratio": 100,
        "expected_runtime_seconds": 25
      },
      "dependencies": ["sha256sum"]
    },
    {
      "id": "sort_compress",
      "name": "Sort and Compress with Skew",
      "description": "Log processing with one large chunk among small ones",
      "category": "data_processing",
      "setup_commands": [
        "for i in {1..99}; do seq 1 1000 | shuf > part_$i.tsv; done",
        "seq 1 50000000 | shuf > part_100.tsv"
      ],
      "small_commands": [
        "sort part_{ID}.tsv | zstd -q -o part_{ID}.tsv.zst"
      ],
      "large_commands": [
        "sort part_100.tsv | zstd -q -o part_100.tsv.zst"
      ],
      "test_command": "parallel -j2 --line-buffer 'sort {} | zstd -q -o {}.zst' ::: part_*.tsv",
      "cleanup_commands": [
        "rm -f part_*.tsv part_*.tsv.zst"
      ],
      "expected_improvement": 0.30,
      "workload_characteristics": {
        "small_tasks": 99,
        "large_tasks": 1,
        "size_ratio": 100,
        "expected_runtime_seconds": 18
      },
      "dependencies": ["parallel", "zstd"]
    },
    {
      "id": "spark_shuffle",
      "name": "Spark Local Shuffle with Skew",
      "description": "Analytics with skewed data distribution (hot key problem)",
      "category": "data_processing",
      "setup_commands": [
        "chmod +x assets/spark_skew_test.py",
        "mkdir -p spark_data",
        "for i in {1..99}; do echo \"partition_$i\" > spark_data/part_$i.txt; done",
        "echo \"large_partition\" > spark_data/part_100.txt"
      ],
      "small_commands": [
        "python3 -c \"import time; time.sleep(0.1); print('Processed partition {ID}')\""
      ],
      "large_commands": [
        "python3 -c \"import time; time.sleep(10); print('Processed large partition')\""
      ],
      "test_command": "python3 assets/spark_skew_test.py",
      "cleanup_commands": [
        "rm -rf spark_data/"
      ],
      "expected_improvement": 0.33,
      "workload_characteristics": {
        "small_tasks": 99,
        "large_tasks": 1,
        "size_ratio": 100,
        "expected_runtime_seconds": 20
      },
      "dependencies": ["python3"]
    },
    {
      "id": "dask_groupby",
      "name": "Dask DataFrame Groupby",
      "description": "Customer analytics with power-law distribution",
      "category": "data_processing",
      "setup_commands": [
        "chmod +x assets/dask_groupby_test.py",
        "mkdir -p dask_data",
        "python3 -c \"import csv; [csv.writer(open(f'dask_data/chunk_{i}.csv', 'w')).writerows([['key', 'value'], [str(i), '1']]) for i in range(1, 100)]\"",
        "python3 -c \"import csv; w = csv.writer(open('dask_data/chunk_100.csv', 'w')); w.writerow(['key', 'value']); [w.writerow(['999', str(i)]) for i in range(1000000)]\""
      ],
      "small_commands": [
        "python3 -c \"import pandas as pd; import time; time.sleep(0.001); print(f'Processed chunk {ID}')\""
      ],
      "large_commands": [
        "python3 -c \"import pandas as pd; import time; start = time.time(); df = pd.read_csv('dask_data/chunk_100.csv'); for i in range(50): groups = df.groupby('key').agg({'value': ['count', 'sum', 'mean', 'std', 'min', 'max']}).reset_index(); time.sleep(0.1); print(f'Grouped large chunk: {len(groups)} groups in {time.time()-start:.2f}s')\""
      ],
      "test_command": "python3 assets/dask_groupby_test.py",
      "cleanup_commands": [
        "rm -rf dask_data/"
      ],
      "expected_improvement": 0.35,
      "workload_characteristics": {
        "small_tasks": 99,
        "large_tasks": 1,
        "size_ratio": 100,
        "expected_runtime_seconds": 22
      },
      "dependencies": ["python3", "python3-pandas", "python3-numpy"]
    },
    {
      "id": "pandas_etl",
      "name": "Pandas Multiprocessing ETL",
      "description": "Log processing with DDoS spike creating large file",
      "category": "data_processing",
      "setup_commands": [
        "mkdir -p logs",
        "for i in {1..99}; do seq 1 100 | awk '{print strftime(\"%Y-%m-%d %H:%M:%S\"), \"INFO\", \"Message\", $1}' | gzip > logs/log$i.gz; done",
        "seq 1 1000000 | awk '{print strftime(\"%Y-%m-%d %H:%M:%S\"), \"INFO\", \"Message\", $1}' | gzip > logs/log100.gz",
        "chmod +x assets/pandas_etl_test.py"
      ],
      "small_commands": [
        "python3 -c \"import gzip; import pandas as pd; import time; time.sleep(0.002); print(f'Processed log{ID}')\""
      ],
      "large_commands": [
        "python3 -c \"import gzip; import pandas as pd; import time; start = time.time(); lines = gzip.open('logs/log100.gz', 'rt').readlines(); df = pd.DataFrame([l.split() for l in lines[:100000]]); for i in range(20): result = df.groupby(0).agg({1: ['count', 'nunique'], 2: ['count']}).reset_index(); time.sleep(0.2); print(f'Processed log100: {len(lines)} lines in {time.time()-start:.2f}s')\""
      ],
      "test_command": "python3 assets/pandas_etl_test.py",
      "cleanup_commands": [
        "rm -rf logs/"
      ],
      "expected_improvement": 0.35,
      "workload_characteristics": {
        "small_tasks": 99,
        "large_tasks": 1,
        "size_ratio": 100,
        "expected_runtime_seconds": 20
      },
      "dependencies": ["python3", "python3-pandas", "gzip"]
    },
    {
      "id": "flink_join",
      "name": "Local Flink Batch Join",
      "description": "Retail analytics with popular items creating join skew",
      "category": "data_processing",
      "setup_commands": [
        "chmod +x assets/flink_join_test.py",
        "mkdir -p flink_data",
        "for i in {1..99}; do echo \"key_$i,value_$i\" > flink_data/join_$i.txt; done",
        "python3 -c \"with open('flink_data/join_100.txt', 'w') as f: [f.write(f'key_999,value_{i}\\n') for i in range(500000)]\""
      ],
      "small_commands": [
        "python3 -c \"import time; time.sleep(0.001); print(f'Joined partition {ID}')\""
      ],
      "large_commands": [
        "python3 -c \"import time; start = time.time(); lines = open('flink_data/join_100.txt').readlines(); data = [l.strip().split(',') for l in lines]; for i in range(50): joined = [(k, len([x for x in data if x[0] == k])) for k in set([x[0] for x in data])]; time.sleep(0.1); print(f'Joined large partition: {len(lines)} records, {len(joined)} keys in {time.time()-start:.2f}s')\""
      ],
      "test_command": "python3 assets/flink_join_test.py",
      "cleanup_commands": [
        "rm -rf flink_data/"
      ],
      "expected_improvement": 0.30,
      "workload_characteristics": {
        "small_tasks": 99,
        "large_tasks": 1,
        "size_ratio": 100,
        "expected_runtime_seconds": 18
      },
      "dependencies": ["python3"]
    }
  ],
  "metadata": {
    "version": "2.0",
    "description": "Long-tail workload test cases with parallel command definitions",
    "target_platform": "Linux",
    "cpu_count": 4,
    "total_test_cases": 10,
    "average_expected_improvement": 0.32
  }
}